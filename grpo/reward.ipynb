{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d383bcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb\n",
    "import statistics\n",
    "from datasets import load_dataset, Dataset\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from trl import GRPOConfig, GRPOTrainer\n",
    "\n",
    "# 1) Initialize W&B\n",
    "wandb.init(project=\"my_grpo_project\", name=\"run-grpo-with-rewards\")\n",
    "model=finetune_model\n",
    "\n",
    "# 3) Prepare a dataset of raw-text prompts\n",
    "#    Here we use wikitext-2-raw-v1 and take the first 100 non-empty lines.\n",
    "raw = load_dataset(\"wikitext\", \"wikitext-2-raw-v1\", split=\"train[:1%]\")\n",
    "# Keep only the 'text' column, rename it to 'prompt', and filter out blanks\n",
    "ds = raw.remove_columns([c for c in raw.column_names if c != \"text\"])\n",
    "ds = ds.rename_column(\"text\", \"prompt\")\n",
    "ds = ds.filter(lambda x: x[\"prompt\"].strip() != \"\")  # drop empty prompts\n",
    "# Take a small subset for demo\n",
    "train_dataset = ds.shuffle(seed=42).select(range(100))\n",
    "\n",
    "# 4) Your custom scoring function\n",
    "def my_custom_score(text: str) -> float:\n",
    "    # Replace with your real reward logic\n",
    "    return float(len(text))\n",
    "\n",
    "# 5) TRL-compatible reward fn that logs each step to console & W&B\n",
    "def my_reward_fn(completions, **kwargs):\n",
    "    step = kwargs.get(\"step\", None)\n",
    "    rewards = [my_custom_score(gen) for gen in completions]\n",
    "\n",
    "    # Console print\n",
    "    print(f\"[GRPO step {step}] rewards = {rewards}\")\n",
    "\n",
    "    # W&B Table\n",
    "    table = wandb.Table(columns=[\"step\", \"sample_idx\", \"reward\", \"generation\"])\n",
    "    for idx, (r, gen) in enumerate(zip(rewards, completions)):\n",
    "        table.add_data(step, idx, r, gen)\n",
    "    wandb.log({\"reward_table\": table}, step=step)\n",
    "\n",
    "    # Aggregate scalars\n",
    "    wandb.log({\n",
    "        \"reward_mean\": sum(rewards) / len(rewards),\n",
    "        \"reward_std\": statistics.stdev(rewards) if len(rewards) > 1 else 0.0,\n",
    "    }, step=step)\n",
    "\n",
    "    return rewards\n",
    "\n",
    "# 6) Configure GRPO: ensure batch sizes are compatible\n",
    "config = GRPOConfig(\n",
    "    output_dir               = \"./grpo_outputs\",\n",
    "    report_to                = \"wandb\",\n",
    "    logging_strategy         = \"steps\",\n",
    "    logging_steps            = 1,\n",
    "    # per_device_train_batch_size = 4,   # effective batch size = 4\n",
    "    # generation_batch_size    = 4,      # must divide 4 evenly\n",
    ")\n",
    "\n",
    "# 7) Instantiate and run the trainer\n",
    "trainer = GRPOTrainer(\n",
    "    model         = model,\n",
    "    processing_class     = tokenizer,      # explicitly pass tokenizer\n",
    "    reward_funcs  = my_reward_fn,\n",
    "    args          = config,\n",
    "    train_dataset = train_dataset,  # each example has a 'prompt' key now\n",
    ")\n",
    "trainer.train()\n",
    "\n",
    "# 8) Finish W&B run\n",
    "wandb.finish()\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
