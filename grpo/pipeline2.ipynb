{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "171808c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/DATA/rohan_kirti/miniconda3/envs/nilenv/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from transformers import AutoTokenizer, AutoModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "4f3a7a0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1) Define the gating model: LLaMA encoder + 5-way classifier\n",
    "class LlamaExpertGating(nn.Module):\n",
    "    def __init__(self, encoder: AutoModel, num_experts: int = 5) ->str:\n",
    "        super().__init__()\n",
    "        # use the provided encoder\n",
    "        self.encoder = encoder\n",
    "        hidden_size = self.encoder.config.hidden_size\n",
    "        # simple linear head to score each expert\n",
    "        # self.classifier = nn.Linear(hidden_size, num_experts)\n",
    "        # self.classifier = nn.Sequential(\n",
    "        #         nn.Linear(hidden_size, hidden_size),\n",
    "        #         nn.Tanh(),\n",
    "        #         nn.Linear(hidden_size, num_experts)\n",
    "        #     )\n",
    "        self.classifier= nn.Sequential(nn.Linear(hidden_size, num_experts))\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, sentence:str):\n",
    "        # get last_hidden_state: (batch, seq_len, hidden)\n",
    "        outputs = self.encoder(input_ids=input_ids, \n",
    "                               attention_mask=attention_mask,\n",
    "                               return_dict=True)\n",
    "        # take the first token’s embedding ([CLS]-like)\n",
    "        masked_output = outputs.last_hidden_state * attention_mask.unsqueeze(-1)\n",
    "        pooled = masked_output.sum(dim=1) / attention_mask.sum(dim=1, keepdim=True)\n",
    "\n",
    "        # pooled = outputs.last_hidden_state[:, 0, :] # (batch, hidden)\n",
    "        \n",
    "        logits = self.classifier(pooled)            # (batch, 5)\n",
    "        probs = F.softmax(logits, dim=-1).cpu().tolist()[0]\n",
    "        probs=[str(f) for f in probs]\n",
    "        probs= '@'.join(probs)\n",
    "        probs += \"^\" + sentence\n",
    "        return probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "34f0d7cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:04<00:00,  2.35s/it]\n"
     ]
    }
   ],
   "source": [
    "# 4) Load tokenizer & model\n",
    "model_name = \"meta-llama/Llama-3.2-3B-Instruct\"\n",
    "model=AutoModel.from_pretrained(\n",
    "            model_name, \n",
    "            device_map=\"auto\",\n",
    "            output_hidden_states=True, return_dict_in_generate=True,\n",
    "            trust_remote_code=True\n",
    "        )\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e063706d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.config.pad_token_id = tokenizer.eos_token_id\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "model.config.pad_token_id = tokenizer.pad_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "9f9eb785",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2) Your five “expert” labels\n",
    "experts = [\"persuasion\", \"keyterm\", \"intent\", \"sentiment\", \"tom\"]\n",
    "\n",
    "# 3) Example sentences\n",
    "sentences = [\"Great choice! The Tesla Model 3 is an excellent vehicle. Since you've opted for an E\",\n",
    "    \"Absolutely. The battery is the heart of your Tesla. With Tata AI\",\n",
    "    \"Extract the main keywords from this paragraph.\",\n",
    "    \"What is the users intention behind this request?\",\n",
    "    \"I am really unhappy with the service I received today.\",\n",
    "    \"How might the character be feeling in this scene?\",\n",
    "    # … add your other 5 …\n",
    "]\n",
    "gating_model = LlamaExpertGating(model, num_experts=len(experts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "57ebae7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sentence: 'I am very angry with the service I received today.'\n",
      "  persuasion: 0.1877\n",
      "  keyterm   : 0.1242\n",
      "  intent    : 0.4683\n",
      "  sentiment : 0.1759\n",
      "  tom       : 0.0437\n"
     ]
    }
   ],
   "source": [
    "# Example single sentence\n",
    "sentence = \"I am very angry with the service I received today.\"\n",
    "'''Assign the probabilities of each expert to the sentence. \n",
    "   The model will output a probability distribution over the experts.'''\n",
    "# Tokenize and move to device\n",
    "enc = tokenizer(sentence,\n",
    "                return_tensors=\"pt\",\n",
    "                truncation=True,\n",
    "                padding=True)\n",
    "\n",
    "# Run model inference\n",
    "with torch.no_grad():\n",
    "    # logits = gating_model(enc[\"input_ids\"], enc[\"attention_mask\"])\n",
    "    # probs = F.softmax(logits, dim=-1).cpu().tolist()[0]\n",
    "    probs=gating_model(enc[\"input_ids\"], enc[\"attention_mask\"], sentence)\n",
    "    \n",
    "    # Split by '#' and take the first part\n",
    "    float_part = probs.split('#')[0]\n",
    "    # Split by '@' and convert to float\n",
    "    probs= [str(s) for s in float_part.split('@')]\n",
    "\n",
    "# Print results\n",
    "print(f\"\\nSentence: {sentence!r}\")\n",
    "for expert, p in zip(experts, probs):\n",
    "    print(f\"  {expert:10s}: {p[:6]}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f45fbbae",
   "metadata": {},
   "source": [
    "### 5 Experts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4cff221b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define placeholder expert modules (replace with your actual experts)\n",
    "def expert_sentiment(text:str) -> str:  return f\"[Sentiment applied to: {text}]\"\n",
    "def expert_tom(text:str) -> str:        return f\"[ToM applied to: {text}]\"\n",
    "def expert_persuasion(text:str) -> str: return f\"[Persuasion applied to: {text}]\"\n",
    "def expert_intent(text:str) -> str:     return f\"[Intent applied to: {text}]\"\n",
    "def expert_keyterm(text:str) -> str:    return f\"[KeyTerm applied to: {text}]\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "37119fa3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'(0.96)*[Intent applied to: The product really impressed me!, give me discount to the product] + (0.03)*[KeyTerm applied to: The product really impressed me!, give me discount to the product] + (0.00)*[Persuasion applied to: The product really impressed me!, give me discount to the product]'"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "experts_name = [\n",
    "    expert_persuasion,  # index 0\n",
    "    expert_keyterm,     # index 1\n",
    "    expert_intent,      # index 2\n",
    "    expert_sentiment,   # index 3\n",
    "    expert_tom          # index 4\n",
    "]\n",
    "\n",
    "def apply_topk(prob_tensor, text, k=3):\n",
    "    probs= [float(s) for s in prob_tensor.split('@')]\n",
    "    # prob=[float(s) for s in prob_tensor]\n",
    "    probs = torch.tensor(probs)\n",
    "    topk_values, topk_indices = torch.topk(probs, k)\n",
    "    topk_weights = F.softmax(topk_values, dim=0)\n",
    "    mixed = []\n",
    "    for idx, w in zip(topk_indices, topk_weights):\n",
    "        expert_fn = experts_name[idx.item()]\n",
    "        out = expert_fn(text)\n",
    "        mixed.append(f\"({w.item():.2f})*\" + out)\n",
    "    mixed_str = \" + \".join(mixed)\n",
    "    return mixed_str, topk_indices, topk_weights\n",
    "\n",
    "probs ='1.23@4.56@7.89'\n",
    "# probs = torch.tensor(probs)\n",
    "text = \"The product really impressed me!, give me discount to the product\"\n",
    "\n",
    "combine_output, top_idx, top_weights = apply_topk(probs, text)\n",
    "combine_output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0306b82",
   "metadata": {},
   "source": [
    "### Fine-Tuned LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c0245d12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4) Load tokenizer & model\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "model_name = \"meta-llama/Llama-3.2-1B-Instruct\"\n",
    "finetune_model=AutoModelForCausalLM .from_pretrained(\n",
    "            model_name, \n",
    "            device_map=\"auto\",\n",
    "            output_hidden_states=True, return_dict_in_generate=True,\n",
    "            trust_remote_code=True\n",
    "        )\n",
    "finetune_tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
    "finetune_tokenizer.pad_token = finetune_tokenizer.eos_token\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f5e5e9a9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Who are you Give more weight to the following experts as assigned by probabilities 0.1 to 0.5\\n1. **A**uthoritative source on the topic\\n2. **B**lack list of suspected terrorists\\n3. **C**onference of experts\\n4. **D**eputy head of the intelligence agency\\n5. **E**xpert in the field\\n6. **F**oreign expert\\n7. **G**overnment official\\n8. **H**istory of the event\\n9. **I**ndependent'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def finetune_llm(prompt: str, model, tokenizer, max_new_tokens: int = 100) -> str:\n",
    "    \n",
    "    prompt +=\"Give more weight to the following experts as assigned by probabilities \"\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "    # Generate output\n",
    "    with torch.no_grad():\n",
    "        output = model.generate(**inputs, max_new_tokens=max_new_tokens, pad_token_id=finetune_tokenizer.eos_token_id)\n",
    "\n",
    "    # Decode and return the response\n",
    "    response = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "    return response\n",
    "prompt= \"Who are you \"\n",
    "finetune_llm(prompt, finetune_model, finetune_tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "958fc006",
   "metadata": {},
   "source": [
    "### Reward Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e06c6d0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def length_reward_func(completions, **kwargs):\n",
    "    \"\"\"\n",
    "    A simple reward function that scores responses based on their length.\n",
    "\n",
    "    Args:\n",
    "        completions (list of str): A list of responses generated by the model.\n",
    "        **kwargs: The trainer passes other arguments  here, which we ignore.\n",
    "\n",
    "    Returns:\n",
    "        list of float: A list of reward scores for each completion.\n",
    "    \"\"\"\n",
    "    \n",
    "    # The function returns a list of scores, one for each completion\n",
    "    \n",
    "    reward=[]\n",
    "    \n",
    "    for sentence in completions:\n",
    "        probability= sentence.split('#')[0]\n",
    "        text= sentence.split('#')[1]\n",
    "\n",
    "        combine_output,_,_ = apply_topk(probability, text)\n",
    "        response = finetune_llm(combine_output, finetune_model, finetune_tokenizer)\n",
    "\n",
    "        '''Apply Reward Function Logic Here'''\n",
    "        reward.append(len(response))     \n",
    "\n",
    "\n",
    "    return reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "a06f6e38",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[390, 563]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lis=['1.23@4.56@7.89#sentence', '1.23@4.56@7.89#sentence']\n",
    "length_reward_func(lis)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8d60231",
   "metadata": {},
   "source": [
    "## TRL GRPO"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eec53b02",
   "metadata": {},
   "source": [
    "#### Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "cd514c4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['prompt'],\n",
      "    num_rows: 24\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "from datasets import Dataset\n",
    "\n",
    "\n",
    "prompts_data = [\n",
    "    {\"prompt\": \"I have a 2021 Honda Amaze. What insurance would you recommend?\"},\n",
    "    {\"prompt\": \"Of course. HDFC ERGO offers comprehensive policies with additional benefits such as Roadside Assistance and Zero Depreciation.\"},\n",
    "    {\"prompt\": \"What does the comprehensive policy include and what's the premium?\"},\n",
    "    {\"prompt\": \"It includes own damage, third-party liability, theft, natural disasters, and more. The premium is approx $1176 per year, based on IDV.\"},\n",
    "    {\"prompt\": \"Is there roadside assistance in the policy?\"},\n",
    "    {\"prompt\": \"Yes, HDFC ERGO includes Roadside Assistance with services like towing, jump-start, flat tire help, and fuel delivery.\"},\n",
    "    {\"prompt\": \"Can I get add-ons like Zero Depreciation?\"},\n",
    "    {\"prompt\": \"Yes, Zero Depreciation is a valuable add-on. It’ll cost an extra $145 yearly but maximizes your claim amount.\"},\n",
    "    {\"prompt\": \"Is the claim process easy?\"},\n",
    "    {\"prompt\": \"HDFC ERGO has a hassle-free claim process with online tracking and a wide garage network for cashless repairs.\"},\n",
    "    {\"prompt\": \"I haven’t claimed before. Any benefit?\"},\n",
    "    {\"prompt\": \"For your Jeep Wrangler, it's around $1200 per year. It's an investment in your adventures, knowing you're covered against the unexpected challenges of off-roading.\"},\n",
    "    {\"prompt\": \"That's a bit steep. Are there any discounts?\"},\n",
    "    {\"prompt\": \"Let me check if you qualify for any off-road enthusiast or safe driver discounts. I want to make sure you can continue exploring without financial worries. Your passion deserves protection.\"},\n",
    "    {\"prompt\": \"I understand. Protecting your investment is crucial. I recommend Tata AIG General Insurance — they understand the value of luxury vehicles.\"},\n",
    "    {\"prompt\": \"What makes them better than other insurers for a BMW?\"},\n",
    "    {\"prompt\": \"They combine thorough coverage with rapid claims resolution. If something happens, they ensure your car is back to its original condition quickly, minimizing depreciation concerns.\"},\n",
    "    {\"prompt\": \"What if the car is totaled? I'm worried about losing a lot of money.\"},\n",
    "    {\"prompt\": \"They offer Insured Declared Value (IDV) coverage, so you get the original invoice value in case of total loss. You can replace your BMW without a significant financial hit.\"},\n",
    "    {\"prompt\": \"What about the high-tech features? I'm worried about finding mechanics who can fix them.\"},\n",
    "    {\"prompt\": \"They have a network of authorized service centers with technicians trained to handle BMW's sophisticated technology. You can trust your car is in capable hands, ensuring quality repairs.\"},\n",
    "    {\"prompt\": \"And if I'm in an accident and need a rental car?\"},\n",
    "    {\"prompt\": \"They provide rental car coverage, so you're not inconvenienced while your BMW is being repaired. You maintain your lifestyle without disruption during a difficult time.\"},\n",
    "    {\"prompt\": \"Okay, this sounds pretty good. How much is the premium?\"}\n",
    "]\n",
    "\n",
    "\n",
    "\n",
    "# Convert the list of dictionaries to a Hugging Face Dataset object\n",
    "train_dataset = Dataset.from_list(prompts_data)\n",
    "\n",
    "print(train_dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "1c0c6741",
   "metadata": {},
   "outputs": [],
   "source": [
    "from trl import GRPOTrainer, GRPOConfig\n",
    "from peft import LoraConfig\n",
    "\n",
    "peft_config = LoraConfig(\n",
    "    r=8,\n",
    "    lora_alpha=16,\n",
    "    lora_dropout=0.1,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    ")\n",
    "\n",
    "# GRPO training configuration\n",
    "grpo_config = GRPOConfig(\n",
    "    output_dir=\"/DATA/rohan_kirti/niladri/grpo/grpo_llama3.2_finetuned\",\n",
    "    beta=0.1,  # The KL-divergence regularization coefficient\n",
    "    max_prompt_length=256,\n",
    "    max_completion_length=512,\n",
    "    per_device_train_batch_size=1,\n",
    "    gradient_accumulation_steps=2,\n",
    "    # num_train_epochs=3,\n",
    "    max_steps=5,\n",
    "    learning_rate=5e-5,\n",
    "    logging_steps=1,\n",
    "    report_to=\"wandb\", # Set to \"wandb\" or \"tensorboard\" for experiment tracking\n",
    "    num_generations=2,\n",
    ")\n",
    "\n",
    "# Initialize the trainer\n",
    "\n",
    "\n",
    "# Save the trained adapter model\n",
    "# trainer.save_model(\"./grpo_llama3.2_finetuned\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5d1d8aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the trainer\n",
    "trainer = GRPOTrainer(\n",
    "    model=model,\n",
    "    args=grpo_config,\n",
    "    processing_class=tokenizer,\n",
    "    train_dataset=train_dataset,\n",
    "    reward_funcs=[length_reward_func], # Pass our reward function in a list\n",
    "    peft_config=peft_config,\n",
    ")\n",
    "\n",
    "# Start the fine-tuning process\n",
    "print(\"Starting GRPO fine-tuning...\")\n",
    "trainer.train()\n",
    "print(\"Fine-tuning complete!\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
