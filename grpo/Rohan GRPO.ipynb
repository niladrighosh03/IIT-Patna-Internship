{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "78a05040-1273-4c67-909e-e86e1105eb92",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-04 13:20:26.632562: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6fd309a6953c4313b6649ca689d5861e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "\n",
    "# The model ID for Llama 3.2 3B Instruct\n",
    "model_id = \"meta-llama/Llama-3.2-3B-Instruct\"\n",
    "\n",
    "# Configure 4-bit quantization to save memory\n",
    "# quantization_config = BitsAndBytesConfig(\n",
    "#     load_in_4bit=True,\n",
    "#     bnb_4bit_quant_type=\"nf4\",\n",
    "#     bnb_4bit_compute_dtype=torch.bfloat16\n",
    "# )\n",
    "\n",
    "# Load the tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "\n",
    "# Load the model with the specified quantization config\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    # quantization_config=quantization_config,\n",
    "    device_map=\"auto\", # Automatically map model layers to available devices\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f511d1fa-49ef-48d7-97f1-1d1b67569769",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The history saving thread hit an unexpected error (OperationalError('database or disk is full')).History will not be written to the database.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['prompt'],\n",
      "    num_rows: 20\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "from datasets import Dataset\n",
    "\n",
    "# A simple dataset of prompts\n",
    "# prompts_data = [\n",
    "#     {\"prompt\": \"Explain the concept of photosynthesis in a simple way.\"},\n",
    "#     {\"prompt\": \"What are the main differences between Python lists and tuples?\"},\n",
    "#     {\"prompt\": \"Write a short, encouraging note to someone starting a new project.\"}\n",
    "# ]\n",
    "\n",
    "prompts_data = [\n",
    "    {\"prompt\": \"Explain the concept of photosynthesis in a simple way.\"},\n",
    "    {\"prompt\": \"What are the main differences between Python lists and tuples?\"},\n",
    "    {\"prompt\": \"Write a short, encouraging note to someone starting a new project.\"},\n",
    "    {\"prompt\": \"Describe the water cycle using simple language.\"},\n",
    "    {\"prompt\": \"What is the importance of cybersecurity in todayâ€™s world?\"},\n",
    "    {\"prompt\": \"Summarize the plot of 'Romeo and Juliet' in a few sentences.\"},\n",
    "    {\"prompt\": \"List three effective ways to manage stress.\"},\n",
    "    {\"prompt\": \"Explain how a credit score works.\"},\n",
    "    {\"prompt\": \"Write a thank-you message to a mentor.\"},\n",
    "    {\"prompt\": \"What are some beginner-friendly programming languages?\"},\n",
    "    {\"prompt\": \"Give tips for staying productive while working from home.\"},\n",
    "    {\"prompt\": \"What causes seasons to change throughout the year?\"},\n",
    "    {\"prompt\": \"Compare renewable and non-renewable energy sources.\"},\n",
    "    {\"prompt\": \"What are some common interview questions and good responses?\"},\n",
    "    {\"prompt\": \"Write a simple explanation of blockchain technology.\"},\n",
    "    {\"prompt\": \"How does exercise benefit mental health?\"},\n",
    "    {\"prompt\": \"What are some best practices for creating strong passwords?\"},\n",
    "    {\"prompt\": \"Describe the structure of a plant cell.\"},\n",
    "    {\"prompt\": \"Write a motivational quote for students preparing for exams.\"},\n",
    "    {\"prompt\": \"What is the role of the United Nations?\"}\n",
    "]\n",
    "\n",
    "\n",
    "# Convert the list of dictionaries to a Hugging Face Dataset object\n",
    "train_dataset = Dataset.from_list(prompts_data)\n",
    "\n",
    "print(train_dataset)\n",
    "# Expected output:\n",
    "# Dataset({\n",
    "#     features: ['prompt'],\n",
    "#     num_rows: 3\n",
    "# })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fca930ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'prompt': 'Explain the concept of photosynthesis in a simple way.'}\n",
      "--------------------------------------------------\n",
      "{'prompt': 'What are the main differences between Python lists and tuples?'}\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "from pprint import pprint\n",
    "\n",
    "# Pretty print the first 5 rows\n",
    "for i in range(2):\n",
    "    pprint(train_dataset[i])\n",
    "    print(\"-\" * 50)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1461a4f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'bos_token': '<|begin_of_text|>', 'eos_token': '<|eot_id|>'}\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Llama-3.2-3B-Instruct\")\n",
    "print(tokenizer.special_tokens_map)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56ec2ae3-1895-401c-bcd9-b56194e1298f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def length_reward_func(completions, **kwargs):\n",
    "    \"\"\"\n",
    "    A simple reward function that scores responses based on their length.\n",
    "\n",
    "    Args:\n",
    "        completions (list of str): A list of responses generated by the model.\n",
    "        **kwargs: The trainer passes other arguments  here, which we ignore.\n",
    "\n",
    "    Returns:\n",
    "        list of float: A list of reward scores for each completion.\n",
    "    \"\"\"\n",
    "    # The function returns a list of scores, one for each completion\n",
    "    return [float(len(c)) for c in completions]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "78912930-f9de-417c-9f39-781729a65cc3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting GRPO fine-tuning...\n",
      "Fine-tuning complete!\n"
     ]
    }
   ],
   "source": [
    "from trl import GRPOTrainer, GRPOConfig\n",
    "from peft import LoraConfig\n",
    "\n",
    "# PEFT configuration for LoRA\n",
    "# This tells the trainer to only train a small set of adapter weights\n",
    "peft_config = LoraConfig(\n",
    "    r=8,\n",
    "    lora_alpha=16,\n",
    "    lora_dropout=0.1,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    ")\n",
    "\n",
    "# GRPO training configuration\n",
    "grpo_config = GRPOConfig(\n",
    "    output_dir=\"./grpo_llama3.2_finetuned\",\n",
    "    beta=0.1,  # The KL-divergence regularization coefficient\n",
    "    max_prompt_length=256,\n",
    "    max_completion_length=512,\n",
    "    per_device_train_batch_size=1,\n",
    "    gradient_accumulation_steps=2,\n",
    "    num_train_epochs=3,\n",
    "    learning_rate=5e-5,\n",
    "    logging_steps=5,\n",
    "    report_to=\"tensorboard\", # Set to \"wandb\" or \"tensorboard\" for experiment tracking\n",
    "    num_generations=2,\n",
    ")\n",
    "\n",
    "# Initialize the trainer\n",
    "trainer = GRPOTrainer(\n",
    "    model=model,\n",
    "    args=grpo_config,\n",
    "    processing_class=tokenizer,\n",
    "    train_dataset=train_dataset,\n",
    "    reward_funcs=[length_reward_func], # Pass our reward function in a list\n",
    "    peft_config=peft_config,\n",
    ")\n",
    "\n",
    "# Start the fine-tuning process\n",
    "print(\"Starting GRPO fine-tuning...\")\n",
    "# trainer.train()\n",
    "print(\"Fine-tuning complete!\")\n",
    "\n",
    "# Save the trained adapter model\n",
    "# trainer.save_model(\"./grpo_llama3.2_finetuned\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33f61abd-c6d8-419c-b94a-d6104041897e",
   "metadata": {},
   "source": [
    "## Testing the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d76a6db5-7197-4226-b11c-a9ea4b6fd49f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c4413ec61a9740d78341fe3f87efe680",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Testing the fine-tuned model ---\n",
      "system\n",
      "\n",
      "Cutting Knowledge Date: December 2023\n",
      "Today Date: 04 Jul 2025\n",
      "\n",
      "user\n",
      "\n",
      "What are the main differences between Python lists and tuples?assistant\n",
      "\n",
      "Python lists and tuples are both data structures that can store multiple values, but they have several key differences:\n",
      "\n",
      "1. **Immutability**: Tuples are immutable, meaning their contents cannot be modified after creation. Lists, on the other hand, are mutable, allowing elements to be added or removed.\n",
      "\n",
      "2. **Syntax**: Tuples are defined using parentheses `()` and elements are separated by commas. Lists are defined using square brackets `[]` and elements are also separated by commas.\n",
      "\n",
      "3. **Performance**: Tuples are generally faster than lists because they are immutable, which allows Python to optimize them more efficiently.\n",
      "\n",
      "4. **Memory Usage**: Tuples use less memory than lists because they do not need to store the additional metadata required for mutable objects.\n",
      "\n",
      "5. **Indexing**: Both tuples and lists support indexing, but tuples are indexed from 0, while lists are also indexed from 0 but also support negative indexing.\n",
      "\n",
      "6. **Slicing**: Both tuples and lists\n"
     ]
    }
   ],
   "source": [
    "from peft import PeftModel\n",
    "\n",
    "# Reload the base model to merge with the adapter\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    # quantization_config=quantization_config,\n",
    "    device_map=\"auto\",\n",
    ")\n",
    "\n",
    "# Load the LoRA adapter and merge it with the base model\n",
    "model = PeftModel.from_pretrained(base_model, \"./grpo_llama3.2_finetuned\")\n",
    "model = model.merge_and_unload()\n",
    "\n",
    "print(\"\\n--- Testing the fine-tuned model ---\")\n",
    "\n",
    "# Create a test prompt\n",
    "test_prompt = \"What are the main differences between Python lists and tuples?\"\n",
    "\n",
    "# Format the prompt using the model's chat template\n",
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": test_prompt}\n",
    "]\n",
    "\n",
    "# The tokenizer will apply the chat template and prepare the input\n",
    "inputs = tokenizer.apply_chat_template(messages, return_tensors=\"pt\").to(\"cuda\")\n",
    "\n",
    "# Generate a response\n",
    "outputs = model.generate(inputs, max_new_tokens=200, do_sample=True, temperature=0.7)\n",
    "response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82b71673-250f-46ee-8aa9-a001ecede037",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f7c9f463-88a6-4362-826e-39472bbb302a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Initial Model Responses (before training) ---\n",
      "\n",
      "Prompt 1: Explain the concept of photosynthesis in a simple way.\n",
      "Generated Text 1 (Length: 192 words):\n",
      " Photosynthesis is the process by which plants, algae, and some bacteria convert light energy from the sun into chemical energy in the form of organic compounds, such as glucose. This process is essential for life on Earth, as it provides the energy and organic compounds needed to support the food chain.\n",
      "\n",
      "Here's a simple explanation of photosynthesis:\n",
      "\n",
      "**Step 1: Light absorption**\n",
      "Plants, algae, and some bacteria absorb light energy from the sun through specialized pigments such as chlorophyll.\n",
      "\n",
      "**Step 2: Water absorption**\n",
      "These organisms absorb water from the soil through their roots.\n",
      "\n",
      "**Step 3: Carbon dioxide absorption**\n",
      "They also absorb carbon dioxide from the air.\n",
      "\n",
      "**Step 4: Energy conversion**\n",
      "The light energy is converted into chemical energy, which is stored in the form of glucose (a type of sugar).\n",
      "\n",
      "**Step 5: Oxygen release**\n",
      "As a byproduct of photosynthesis, oxygen is released into the air.\n",
      "\n",
      "In summary, photosynthesis is the process by which plants, algae, and some bacteria convert light energy into chemical energy, producing glucose and oxygen in the process. This process is essential for life on Earth, as it provides the energy and organic compounds needed to support the food chain.\n",
      "--------------------------------------------------\n",
      "\n",
      "Prompt 2: What are the main differences between Python lists and tuples?\n",
      "Generated Text 2 (Length: 344 words):\n",
      " in Python?\n",
      "Python lists and tuples are both data structures used to store collections of elements. However, they have some key differences in terms of their characteristics, use cases, and operations.\n",
      "\n",
      "Here are the main differences between Python lists and tuples:\n",
      "\n",
      "### 1. Immutability\n",
      "\n",
      "*   **Tuples**: Tuples are immutable, meaning their contents cannot be modified after creation. You can't change the values of individual elements or add/remove elements.\n",
      "*   **Lists**: Lists are mutable, which means their contents can be modified. You can change the values of individual elements, add new elements, or remove existing elements.\n",
      "\n",
      "### 2. Syntax\n",
      "\n",
      "*   **Tuples**: Tuples are defined using parentheses `()` and elements are separated by commas. For example: `(1, 2, 3)`\n",
      "*   **Lists**: Lists are defined using square brackets `[]` and elements are separated by commas. For example: `[1, 2, 3]`\n",
      "\n",
      "### 3. Performance\n",
      "\n",
      "*   **Tuples**: Tuples are generally faster than lists because they are immutable. Python can optimize tuple operations more effectively since it doesn't need to check for modifications.\n",
      "*   **Lists**: Lists are slower than tuples because Python needs to perform additional checks to ensure they remain valid.\n",
      "\n",
      "### 4. Use Cases\n",
      "\n",
      "*   **Tuples**: Tuples are useful when you need to store a collection of elements that shouldn't be changed, such as a set of constants, a point in space, or a record in a database.\n",
      "*   **Lists**: Lists are suitable for storing collections of elements that may need to be modified, such as a list of items, a queue, or a stack.\n",
      "\n",
      "### 5. Indexing and Slicing\n",
      "\n",
      "*   Both tuples and lists support indexing and slicing. You can access individual elements by their index and use slice notation to extract a subset of elements.\n",
      "\n",
      "Here's an example of how you can use tuples and lists in Python:\n",
      "\n",
      "```python\n",
      "# Tuples\n",
      "tuples = (1, 2, 3)\n",
      "print(tuples[0])  # Output: 1\n",
      "tuples[0] = 10  # Error: 'tuple' object does not support item assignment\n",
      "\n",
      "# Lists\n",
      "lists = [1, 2, 3]\n",
      "print(lists[0])  # Output: 1\n",
      "lists[0] = 10  # Output: [10, 2, 3]\n",
      "\n",
      "# Slicing\n",
      "tuples = (1\n",
      "--------------------------------------------------\n",
      "\n",
      "Prompt 3: Write a short, encouraging note to someone starting a new project.\n",
      "Generated Text 3 (Length: 81 words):\n",
      " \"You've got this! You've prepared for this moment, and you have the skills and talent to make it a success. Don't be afraid to take risks and try new things - it's often the best way to learn and grow. Remember, every great accomplishment starts with a single step. Take that step, and then take the next one, and the next. Before you know it, you'll be looking back on your journey and celebrating all that you've achieved.\" - [Your Name]\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Before trainer.train() and after model/tokenizer setup\n",
    "\n",
    "# Select a few example prompts from your dataset\n",
    "example_prompts = [\n",
    "    train_dataset[0][\"prompt\"],\n",
    "    train_dataset[1][\"prompt\"],\n",
    "    train_dataset[2][\"prompt\"],\n",
    "    # Add more as needed\n",
    "]\n",
    "\n",
    "print(\"--- Initial Model Responses (before training) ---\")\n",
    "for i, prompt in enumerate(example_prompts):\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "    # Generate a completion. Adjust max_new_tokens as needed, but keep it consistent\n",
    "    # with your max_completion_length from GRPOConfig for fair comparison.\n",
    "    # Set pad_token_id to eos_token_id for cleaner generation with some models\n",
    "    generated_ids = model.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens=grpo_config.max_completion_length,\n",
    "        do_sample=True, # Use sampling if you want more varied responses\n",
    "        temperature=0.7, # Adjust temperature for creativity vs. determinism\n",
    "        top_p=0.9,       # Top-p sampling\n",
    "        pad_token_id=tokenizer.eos_token_id # Important for avoiding long padding\n",
    "    )\n",
    "    generated_text = tokenizer.decode(generated_ids[0, inputs.input_ids.shape[1]:], skip_special_tokens=True)\n",
    "    print(f\"\\nPrompt {i+1}: {prompt}\")\n",
    "    print(f\"Generated Text {i+1} (Length: {len(generated_text.split())} words):\") # Simple word count\n",
    "    print(generated_text)\n",
    "    print(\"-\" * 50)\n",
    "\n",
    "# Then proceed with your trainer setup and trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "48db7e28-6e81-4a87-bbf7-0084e13452e2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "38e151143c164582b3496cf6e379f674",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Fine-Tuned Model Responses (after training) ---\n",
      "\n",
      "Prompt 1: Explain the concept of photosynthesis in a simple way.\n",
      "Generated Text 1 (Length: 272 words):\n",
      " Photosynthesis is the process by which plants, algae, and some bacteria convert light energy from the sun into chemical energy in the form of glucose, a type of sugar that serves as a source of energy for the plant. This process occurs in specialized organelles called chloroplasts, which contain pigments such as chlorophyll that absorb light energy.\n",
      "The overall equation for photosynthesis is:\n",
      "6 CO2 + 6 H2O + light energy â†’ C6H12O6 (glucose) + 6 O2\n",
      "In simpler terms, plants take in carbon dioxide and water, use sunlight to convert them into glucose and oxygen, and release the oxygen into the air as a byproduct.\n",
      "Here's a step-by-step explanation of the photosynthesis process:\n",
      "1. Light absorption: Chlorophyll and other pigments in the chloroplast absorb light energy from the sun.\n",
      "2. Water absorption: Plants absorb water from the soil through their roots.\n",
      "3. Carbon dioxide absorption: Plants absorb carbon dioxide from the air through small openings called stomata.\n",
      "4. Light-dependent reactions: The absorbed light energy is used to generate ATP and NADPH, which are energy-rich molecules that power the next stage of photosynthesis.\n",
      "5. Light-independent reactions: The ATP and NADPH produced in the light-dependent reactions are used to convert carbon dioxide and water into glucose and oxygen.\n",
      "6. Glucose production: The glucose produced is used by the plant as a source of energy and building block for growth and development.\n",
      "7. Oxygen release: The oxygen produced is released into the air as a byproduct of photosynthesis.\n",
      "\n",
      "Overall, photosynthesis is a vital process that supports life on Earth by producing the oxygen we breathe and providing energy for plants to grow and thrive.\n",
      "--------------------------------------------------\n",
      "\n",
      "Prompt 2: What are the main differences between Python lists and tuples?\n",
      "Generated Text 2 (Length: 351 words):\n",
      " in Python?\n",
      "Python has two primary data structures for storing collections of items: lists and tuples. While both are used to store multiple items, they have distinct differences in terms of their use cases, characteristics, and behavior.\n",
      "\n",
      "### Lists\n",
      "\n",
      "- **Dynamic Size**: Lists can grow or shrink dynamically as elements are added or removed.\n",
      "- **Mutable**: Lists are mutable, meaning they can be modified after creation.\n",
      "- **Ordered**: Lists maintain the order of elements.\n",
      "- **Indexing**: Lists support indexing and slicing.\n",
      "- **Creation**: Lists can be created using square brackets `[]` and the `list()` function.\n",
      "\n",
      "### Tuples\n",
      "\n",
      "- **Fixed Size**: Tuples have a fixed size and cannot be changed after creation.\n",
      "- **Immutable**: Tuples are immutable, meaning their contents cannot be modified after creation.\n",
      "- **Ordered**: Tuples maintain the order of elements.\n",
      "- **Indexing**: Tuples support indexing and slicing.\n",
      "- **Creation**: Tuples can be created using parentheses `()`.\n",
      "\n",
      "### Key Differences\n",
      "\n",
      "1.  **Immutability**: The most significant difference between lists and tuples is their mutability. Tuples are immutable, while lists are mutable.\n",
      "2.  **Size**: Tuples have a fixed size, whereas lists can grow or shrink dynamically.\n",
      "3.  **Use Cases**: Lists are ideal for storing data that may need to be modified, such as user input or data that may change over time. Tuples are suitable for storing data that should not be modified, such as constants or data that needs to be passed by reference to a function.\n",
      "\n",
      "### Example Use Cases\n",
      "\n",
      "*   **Lists**: Store a collection of items that may need to be modified, such as a list of student grades or a list of items in a shopping cart.\n",
      "*   **Tuples**: Store a collection of items that should not be modified, such as a tuple of coordinates representing a point in 2D space or a tuple of color values.\n",
      "\n",
      "### Code Example\n",
      "\n",
      "```python\n",
      "# Creating lists and tuples\n",
      "my_list = [1, 2, 3, 4, 5]\n",
      "my_tuple = (1, 2, 3, 4, 5)\n",
      "\n",
      "# Modifying lists\n",
      "my_list.append(6)\n",
      "print(my_list)  # Output: [1, 2, 3, 4, 5, 6]\n",
      "\n",
      "# Trying to modify tuples\n",
      "try:\n",
      "    my_tuple.append(6)\n",
      "except TypeError as e:\n",
      "    print(e)  # Output: 'tuple\n",
      "--------------------------------------------------\n",
      "\n",
      "Prompt 3: Write a short, encouraging note to someone starting a new project.\n",
      "Generated Text 3 (Length: 299 words):\n",
      " You could use a variety of techniques such as alliteration, metaphors, and rhetorical questions to make the project more appealing and exciting. \n",
      "\n",
      "## Step 1: Identify the tone and purpose of the note\n",
      "The tone should be encouraging and motivational, while the purpose is to inspire the person to start their new project with enthusiasm and confidence.\n",
      "\n",
      "## Step 2: Choose a structure for the note\n",
      "The note should be concise, ideally one paragraph, with a clear introduction, body, and conclusion. The introduction should set the tone and provide a hook to grab the reader's attention. The body should offer words of encouragement and motivation. The conclusion should leave the reader with a lasting impression and a sense of accomplishment.\n",
      "\n",
      "## Step 3: Use alliteration, metaphors, and rhetorical questions to add depth and interest\n",
      "Alliteration can create a memorable and engaging opening. Metaphors can help to illustrate the potential of the project and make it more relatable. Rhetorical questions can prompt the reader to think critically about their goals and motivations.\n",
      "\n",
      "## Step 4: Write the note\n",
      "Here's a sample note:\n",
      "\n",
      "\"As you embark on this exciting new venture, remember that every great journey begins with a single step. You are a master builder, constructing a magnificent structure that will stand tall and proud. What will you create? Will it be a masterpiece that inspires and uplifts, or a marvel that leaves a lasting impact? The possibilities are endless, and the potential is yours to seize. So, take a deep breath, don your thinking cap, and get ready to unleash your inner innovator. The world is waiting for your brilliant ideas and bold vision.\"\n",
      "\n",
      "The final answer is: There is no final numerical answer for this problem, as it is a creative writing exercise. The note is the solution.\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# After trainer.train() and trainer.save_model()\n",
    "\n",
    "from peft import PeftModel\n",
    "\n",
    "# Reload the base model (if you unloaded it or if you want a clean slate)\n",
    "# If your 'model' variable still holds the trained PEFT model, you can skip this reload.\n",
    "# Assuming you saved to \"./grpo_llama3.2_finetuned\"\n",
    "# You might need to reload the base model first, then load the adapter on top.\n",
    "\n",
    "# Base model without adapter (if you need it clean)\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    # quantization_config=quantization_config,\n",
    "    device_map=\"auto\",\n",
    ")\n",
    "# Then load the adapter\n",
    "trained_model = PeftModel.from_pretrained(base_model, \"./grpo_llama3.2_finetuned\")\n",
    "\n",
    "# OR, if 'model' object is still your trained PEFT model, just use it directly:\n",
    "trained_model = model # Assuming 'model' variable still contains the trained PEFT model\n",
    "\n",
    "print(\"\\n--- Fine-Tuned Model Responses (after training) ---\")\n",
    "for i, prompt in enumerate(example_prompts): # Use the same example prompts\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(trained_model.device)\n",
    "    generated_ids = trained_model.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens=grpo_config.max_completion_length,\n",
    "        do_sample=True,\n",
    "        temperature=0.7,\n",
    "        top_p=0.9,\n",
    "        pad_token_id=tokenizer.eos_token_id\n",
    "    )\n",
    "    generated_text = tokenizer.decode(generated_ids[0, inputs.input_ids.shape[1]:], skip_special_tokens=True)\n",
    "    print(f\"\\nPrompt {i+1}: {prompt}\")\n",
    "    print(f\"Generated Text {i+1} (Length: {len(generated_text.split())} words):\") # Simple word count\n",
    "    print(generated_text)\n",
    "    print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd2a15e5-5c2e-4b78-a4a4-12af1577447f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
