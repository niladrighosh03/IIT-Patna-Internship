{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "617e3125",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/DATA/rohan_kirti/miniconda3/envs/nilenv/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:04<00:00,  2.21s/it]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "\n",
    "# The model ID for Llama 3.2 3B Instruct\n",
    "model_id = \"meta-llama/Llama-3.2-3B-Instruct\"\n",
    "\n",
    "# Load the tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "\n",
    "# Load the model with the specified quantization config\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    # quantization_config=quantization_config,\n",
    "    device_map=\"auto\", # Automatically map model layers to available devices\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "22a2efaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.config.pad_token_id = tokenizer.eos_token_id\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "model.config.pad_token_id = tokenizer.pad_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dbe6d43d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/DATA/rohan_kirti/miniconda3/envs/nilenv/lib/python3.11/site-packages/transformers/generation/utils.py:2479: UserWarning: You are calling .generate() with the `input_ids` being on a device type different than your model's device. `input_ids` is on cpu, whereas the model is on cuda. You may experience unexpected behaviors or slower generation. Please make sure that you have put `input_ids` to the correct device by calling for example input_ids = input_ids.to('cuda') before running `.generate()`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Add the word 'Niladri' to the begging of the sentence hey bro, what's up?\n",
      "Hey bro, what's up? Niladri's been keeping me busy with his new business venture.\n",
      "Niladri's been keeping me busy with his new business venture. He's been working tirelessly to make it a success and I'm proud of him for taking the leap. \n",
      "Niladri's been keeping me busy with his new business venture. He's been working tirelessly to make it a success and I'm proud of him for taking the leap. I'm just\n"
     ]
    }
   ],
   "source": [
    "sentence  = \"Add the word 'Niladri' to the begging of the sentence hey bro\"  \n",
    "inputs = tokenizer(sentence, return_tensors=\"pt\")\n",
    "\n",
    "# Generate output\n",
    "with torch.no_grad():\n",
    "    outputs = model.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens=100,\n",
    "        do_sample=True,\n",
    "        temperature=0.7,\n",
    "        top_p=0.9,\n",
    "        pad_token_id=tokenizer.pad_token_id \n",
    "    )\n",
    "    response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "83dfd37b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/DATA/rohan_kirti/miniconda3/envs/nilenv/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['prompt'],\n",
      "    num_rows: 25748\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from datasets import Dataset\n",
    "\n",
    "# Load the CSV file\n",
    "df = pd.read_csv('/DATA/rohan_kirti/niladri/grpo/all_conversations.csv')  # Replace 'your_file.csv' with your actual file name or path\n",
    "\n",
    "# Extract only the 'utterance' column and drop rows with missing values\n",
    "utterances = df['utterance'].dropna().tolist()\n",
    "\n",
    "# Convert the utterances into the desired format\n",
    "prompts_data = [{\"prompt\": utterance} for utterance in utterances]\n",
    "\n",
    "# Convert the list of dictionaries to a Hugging Face Dataset object\n",
    "train_dataset = Dataset.from_list(prompts_data)\n",
    "\n",
    "print(train_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "697fe2ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "small_split = train_dataset.train_test_split(test_size=0.1, seed=42)\n",
    "train_dataset = small_split[\"train\"]\n",
    "test_dataset = small_split[\"test\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d908e472",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def strict_format_reward_func(completions, **kwargs) -> list[float]:\n",
    "    \"\"\"Reward function that checks if the completion has a specific strict format.\"\"\"\n",
    "    \n",
    "    response_list = []\n",
    "    for sentence in completions:\n",
    "        sentence = \"Add the word 'Niladri' to the beginning of the sentence and regenerate the response: \" + sentence\n",
    "        inputs = tokenizer(sentence, return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = model.generate(\n",
    "                **inputs,\n",
    "                max_new_tokens=100,\n",
    "                do_sample=True,\n",
    "                temperature=0.7,\n",
    "                top_p=0.9,\n",
    "                pad_token_id=tokenizer.pad_token_id \n",
    "            )\n",
    "\n",
    "        response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "        response_list.append(response)\n",
    "\n",
    "    # Replace original list contents\n",
    "    completions[:] = response_list\n",
    "    \n",
    "    \n",
    "    pattern = r\"^<reasoning>\\n.*?\\n</reasoning>\\n<answer>\\n.*?\\n</answer>\\n$\"\n",
    "    matches = [re.match(pattern, c.strip()) for c in completions]\n",
    "    return [0.5 if match else 0.0 for match in matches]\n",
    "\n",
    "def soft_format_reward_func(completions, **kwargs) -> list[float]:\n",
    "    \"\"\"Reward function that checks if the completion has a loosely correct format.\"\"\"\n",
    "    \n",
    "    \n",
    "    response_list = []\n",
    "    for sentence in completions:\n",
    "        sentence = \"Add the word 'Niladri' to the beginning of the sentence and regenerate the response: \" + sentence\n",
    "        inputs = tokenizer(sentence, return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = model.generate(\n",
    "                **inputs,\n",
    "                max_new_tokens=100,\n",
    "                do_sample=True,\n",
    "                temperature=0.7,\n",
    "                top_p=0.9,\n",
    "                pad_token_id=tokenizer.pad_token_id \n",
    "            )\n",
    "\n",
    "        response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "        response_list.append(response)\n",
    "\n",
    "    # Replace original list contents\n",
    "    completions[:] = response_list\n",
    "    \n",
    "    pattern = r\"<reasoning>.*?</reasoning>\\s*<answer>.*?</answer>\"\n",
    "    matches = [re.search(pattern, c.strip(), re.DOTALL) for c in completions]\n",
    "    return [0.5 if match else 0.0 for match in matches]\n",
    "\n",
    "def count_xml(text: str) -> float:\n",
    "    \"\"\"Internal utility to assign partial scores for XML-like formatting.\"\"\"\n",
    "    count = 0.0\n",
    "    if text.count(\"<reasoning>\\n\") == 1:\n",
    "        count += 0.125\n",
    "    if text.count(\"\\n</reasoning>\\n\") == 1:\n",
    "        count += 0.125\n",
    "    if text.count(\"\\n<answer>\\n\") == 1:\n",
    "        count += 0.125\n",
    "        count -= len(text.split(\"\\n</answer>\\n\")[-1]) * 0.001\n",
    "    if text.count(\"\\n</answer>\") == 1:\n",
    "        count += 0.125\n",
    "        count -= (len(text.split(\"\\n</answer>\")[-1]) - 1) * 0.001\n",
    "    return count\n",
    "\n",
    "def xmlcount_reward_func(completions, **kwargs) -> list[float]:\n",
    "    \"\"\"Reward function giving partial score for structural XML-like format.\"\"\"\n",
    "    response_list = []\n",
    "    for sentence in completions:\n",
    "        sentence = \"Add the word 'Niladri' to the beginning of the sentence and regenerate the response: \" + sentence\n",
    "        inputs = tokenizer(sentence, return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = model.generate(\n",
    "                **inputs,\n",
    "                max_new_tokens=100,\n",
    "                do_sample=True,\n",
    "                temperature=0.7,\n",
    "                top_p=0.9,\n",
    "                pad_token_id=tokenizer.pad_token_id \n",
    "            )\n",
    "\n",
    "        response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "        response_list.append(response)\n",
    "\n",
    "    # Replace original list contents\n",
    "    completions[:] = response_list\n",
    "    return [count_xml(c.strip()) for c in completions]\n",
    "\n",
    "\n",
    "def length_reward_func(completions, **kwargs):\n",
    "    \"\"\"\n",
    "    A simple reward function that scores responses based on their length.\n",
    "\n",
    "    Args:\n",
    "        completions (list of str): A list of responses generated by the model.\n",
    "        **kwargs: The trainer passes other arguments  here, which we ignore.\n",
    "\n",
    "    Returns:\n",
    "        list of float: A list of reward scores for each completion.\n",
    "    \"\"\"\n",
    "    # The function returns a list of scores, one for each completion\n",
    "    response_list=list()\n",
    "    for sentence in completions:\n",
    "        sentence  = \"Add the word 'Niladri' to the begging of the sentence and regenerate the response\"  + sentence\n",
    "        inputs = tokenizer(sentence, return_tensors=\"pt\")\n",
    "\n",
    "        # Generate output\n",
    "        with torch.no_grad():\n",
    "            outputs = model.generate(\n",
    "                **inputs,\n",
    "                max_new_tokens=100,\n",
    "                do_sample=True,\n",
    "                temperature=0.7,\n",
    "                top_p=0.9,\n",
    "                pad_token_id=tokenizer.pad_token_id \n",
    "            )\n",
    "\n",
    "        # Decode and print\n",
    "        response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "        response_list.append(response)\n",
    "        \n",
    "    completions[:] = response_list\n",
    "    \n",
    "\n",
    "    return [float(len(c)) for c in response_list]\n",
    "\n",
    "\n",
    "def keyword_reward_func(completions, **kwargs):\n",
    "    \"\"\"\n",
    "    Reward function that scores responses based on presence of specific persuasive/helpful keywords.\n",
    "    \n",
    "    Args:\n",
    "        completions (list of str): A list of responses generated by the model.\n",
    "        **kwargs: Additional arguments passed by the trainer (ignored here).\n",
    "    \n",
    "    Returns:\n",
    "        list of float: A list of reward scores based on keyword matches.\n",
    "    \"\"\"\n",
    "    \n",
    "    response_list = []\n",
    "    for sentence in completions:\n",
    "        sentence = \"Add the word 'Niladri' to the beginning of the sentence and regenerate the response: \" + sentence\n",
    "        inputs = tokenizer(sentence, return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = model.generate(\n",
    "                **inputs,\n",
    "                max_new_tokens=100,\n",
    "                do_sample=True,\n",
    "                temperature=0.7,\n",
    "                top_p=0.9,\n",
    "                pad_token_id=tokenizer.pad_token_id \n",
    "            )\n",
    "\n",
    "        response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "        response_list.append(response)\n",
    "\n",
    "    # Replace original list contents\n",
    "    completions[:] = response_list\n",
    "    keywords = {\"persuasion\", \"discount\", \"help\", \"offer\", \"support\", \"assist\", \"save\", \"deal\"}\n",
    "    \n",
    "    rewards = []\n",
    "    for c in completions:\n",
    "        lowered = c.lower()\n",
    "        hits = sum(1 for word in keywords if word in lowered)\n",
    "        # Reward = base + 0.2 per keyword match, capped at 1.0\n",
    "        reward = min(1.0, 0.2 * hits)\n",
    "        rewards.append(reward)\n",
    "    \n",
    "    return rewards\n",
    "\n",
    "\n",
    "def keyword_avoidance_reward_func(completions, **kwargs):\n",
    "    response_list = []\n",
    "    for sentence in completions:\n",
    "        sentence = \"Add the word 'Niladri' to the beginning of the sentence and regenerate the response: \" + sentence\n",
    "        inputs = tokenizer(sentence, return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = model.generate(\n",
    "                **inputs,\n",
    "                max_new_tokens=100,\n",
    "                do_sample=True,\n",
    "                temperature=0.7,\n",
    "                top_p=0.9,\n",
    "                pad_token_id=tokenizer.pad_token_id \n",
    "            )\n",
    "\n",
    "        response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "        response_list.append(response)\n",
    "\n",
    "    # Replace original list contents\n",
    "    completions[:] = response_list\n",
    "    bad_keywords = {\"error\", \"unsure\", \"don't know\", \"not possible\"}\n",
    "    return [\n",
    "        0.0 if any(bad in c.lower() for bad in bad_keywords) else 1.0\n",
    "        for c in completions\n",
    "    ]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c972651",
   "metadata": {},
   "outputs": [],
   "source": [
    "from trl import GRPOTrainer, GRPOConfig\n",
    "from peft import LoraConfig\n",
    "\n",
    "peft_config = LoraConfig(\n",
    "    r=8,\n",
    "    lora_alpha=16,\n",
    "    lora_dropout=0.1,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    ")\n",
    "\n",
    "# GRPO training configuration\n",
    "grpo_config = GRPOConfig(\n",
    "    output_dir=\"/DATA/rohan_kirti/niladri/grpo/main\",\n",
    "    beta=0.1,  # The KL-divergence regularization coefficient\n",
    "    max_prompt_length=256,\n",
    "    max_completion_length=512,\n",
    "    per_device_train_batch_size=1,\n",
    "    gradient_accumulation_steps=2,\n",
    "    num_train_epochs=700,\n",
    "    # max_steps=5,\n",
    "    learning_rate=5e-5,\n",
    "    logging_steps=35,\n",
    "    report_to=\"tensorboard\", # Set to \"wandb\" or \"tensorboard\" for experiment tracking\n",
    "    num_generations=2,\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91da4b84",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting GRPO fine-tuning...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/DATA/rohan_kirti/miniconda3/envs/nilenv/lib/python3.11/site-packages/transformers/generation/utils.py:2479: UserWarning: You are calling .generate() with the `input_ids` being on a device type different than your model's device. `input_ids` is on cpu, whereas the model is on cuda. You may experience unexpected behaviors or slower generation. Please make sure that you have put `input_ids` to the correct device by calling for example input_ids = input_ids.to('cuda') before running `.generate()`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='5' max='5' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [5/5 05:21, Epoch 0/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fine-tuning complete!\n"
     ]
    }
   ],
   "source": [
    "# Initialize the trainer\n",
    "trainer = GRPOTrainer(\n",
    "    model=model,\n",
    "    args=grpo_config,\n",
    "    processing_class=tokenizer,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=test_dataset,\n",
    "    reward_funcs=[length_reward_func, keyword_avoidance_reward_func, strict_format_reward_func,\n",
    "                  soft_format_reward_func, xmlcount_reward_func, keyword_reward_func], # Pass our reward function in a list\n",
    "    peft_config=peft_config,\n",
    "    \n",
    ")\n",
    "\n",
    "# Start the fine-tuning process\n",
    "print(\"Starting GRPO fine-tuning...\")\n",
    "trainer.train()\n",
    "print(\"Fine-tuning complete!\")  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f6ad7a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the trained adapter model\n",
    "trainer.save_model(\"/DATA/rohan_kirti/niladri/grpo/main/grpo_llama3.2_finetuned\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
