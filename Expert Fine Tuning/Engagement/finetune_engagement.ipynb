{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8f59eb6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from datasets import load_dataset\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    TrainingArguments,\n",
    "    BitsAndBytesConfig,\n",
    "    Trainer\n",
    "    \n",
    ")\n",
    "import torch.nn.functional as F\n",
    "from peft import LoraConfig, PeftModel\n",
    "from trl import SFTTrainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cbae2f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The model that you want to train from the Hugging Face hub\n",
    "model_name = \"Qwen/Qwen2.5-3B-Instruct\"\n",
    "\n",
    "\n",
    "################################################################################\n",
    "# QLoRA parameters\n",
    "################################################################################\n",
    "\n",
    "# LoRA attention dimension\n",
    "lora_r = 64\n",
    "\n",
    "# Alpha parameter for LoRA scaling\n",
    "lora_alpha = 16\n",
    "\n",
    "# Dropout probability for LoRA layers\n",
    "lora_dropout = 0.1\n",
    "\n",
    "################################################################################\n",
    "# bitsandbytes parameters\n",
    "################################################################################\n",
    "\n",
    "# Activate 4-bit precision base model loading\n",
    "use_4bit = True\n",
    "\n",
    "# Compute dtype for 4-bit base models\n",
    "bnb_4bit_compute_dtype = \"float16\"\n",
    "\n",
    "# Quantization type (fp4 or nf4)\n",
    "bnb_4bit_quant_type = \"nf4\"\n",
    "\n",
    "# Activate nested quantization for 4-bit base models (double quantization)\n",
    "use_nested_quant = False\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "################################################################################\n",
    "# TrainingArguments parameters\n",
    "################################################################################\n",
    "\n",
    "# Output directory where the model predictions and checkpoints will be stored\n",
    "output_dir = \"/DATA/rohan_kirti/niladri/fine_tune/train_dataset/results\"\n",
    "\n",
    "# Number of training epochs\n",
    "num_train_epochs =1\n",
    "\n",
    "# Enable fp16/bf16 training (set bf16 to True with an A100)\n",
    "fp16 = False\n",
    "bf16 = False\n",
    "\n",
    "# Batch size per GPU for training\n",
    "per_device_train_batch_size = 4\n",
    "\n",
    "# Batch size per GPU for evaluation\n",
    "per_device_eval_batch_size = 4\n",
    "\n",
    "# Number of update steps to accumulate the gradients for\n",
    "gradient_accumulation_steps = 1\n",
    "\n",
    "# Enable gradient checkpointing\n",
    "gradient_checkpointing = True\n",
    "\n",
    "# Maximum gradient normal (gradient clipping)\n",
    "max_grad_norm = 0.3\n",
    "\n",
    "# Initial learning rate (AdamW optimizer)\n",
    "learning_rate = 2e-4\n",
    "\n",
    "# Weight decay to apply to all layers except bias/LayerNorm weights\n",
    "weight_decay = 0.001\n",
    "\n",
    "# Optimizer to use\n",
    "optim = \"paged_adamw_32bit\"\n",
    "\n",
    "# Learning rate schedule\n",
    "lr_scheduler_type = \"cosine\"\n",
    "\n",
    "# Number of training steps (overrides num_train_epochs)\n",
    "max_steps = -1\n",
    "\n",
    "# Ratio of steps for a linear warmup (from 0 to learning rate)\n",
    "warmup_ratio = 0.03\n",
    "\n",
    "# Group sequences into batches with same length\n",
    "# Saves memory and speeds up training considerably\n",
    "group_by_length = True\n",
    "\n",
    "# Save checkpoint every X updates steps\n",
    "save_steps = 0\n",
    "\n",
    "# Log every X updates steps\n",
    "logging_steps =2\n",
    "\n",
    "################################################################################\n",
    "# SFT parameters\n",
    "################################################################################\n",
    "\n",
    "# Maximum sequence length to use\n",
    "max_seq_length = None\n",
    "\n",
    "# Pack multiple short examples in the same input sequence to increase efficiency\n",
    "packing = False\n",
    "\n",
    "# Load the entire model on the GPU 0\n",
    "device_map ='auto'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "77e4a6f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating train split: 13383 examples [00:00, 42043.10 examples/s]\n",
      "Map: 100%|██████████| 13383/13383 [00:01<00:00, 12721.09 examples/s]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# 1. Load dataset\n",
    "\n",
    "dataset = load_dataset(\"csv\", data_files=\"/DATA/rohan_kirti/niladri/Expert Fine Tuning/Engagement/Persuasion_results.csv\")\n",
    "\n",
    "# Suppose dataset columns: conversation_id, turn_no, speaker, utterance, new_agent_reply, label, reason\n",
    "labels = list(set(dataset[\"train\"][\"label\"]))\n",
    "label2id = {l: i for i, l in enumerate(labels)}\n",
    "id2label = {i: l for l, i in label2id.items()}\n",
    "\n",
    "def preprocess(example):\n",
    "    # numericalize label\n",
    "    example[\"label_id\"] = label2id[example[\"label\"]]\n",
    "    return example\n",
    "\n",
    "dataset = dataset.map(preprocess)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc4f9076",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Load tokenizer and model with QLoRA configuration\n",
    "compute_dtype = getattr(torch, bnb_4bit_compute_dtype)\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=use_4bit,\n",
    "    bnb_4bit_quant_type=bnb_4bit_quant_type,\n",
    "    bnb_4bit_compute_dtype=compute_dtype,\n",
    "    bnb_4bit_use_double_quant=use_nested_quant,\n",
    ")\n",
    "\n",
    "# Check GPU compatibility with bfloat16\n",
    "if compute_dtype == torch.float16 and use_4bit:\n",
    "    major, _ = torch.cuda.get_device_capability()\n",
    "    if major >= 8:\n",
    "        print(\"=\" * 80)\n",
    "        print(\"Your GPU supports bfloat16: accelerate training with bf16=True\")\n",
    "        print(\"=\" * 80)\n",
    "\n",
    "\n",
    "# Load LoRA configuration\n",
    "peft_config = LoraConfig(\n",
    "    lora_alpha=lora_alpha,\n",
    "    lora_dropout=lora_dropout,\n",
    "    r=lora_r,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2d30045",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 2. Tokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "def tokenize(example):\n",
    "    # Input context\n",
    "    input_enc = tokenizer(example[\"new_agent_reply\"], truncation=True, max_length=256)\n",
    "\n",
    "    # Reasoning supervision: \"reason\" text\n",
    "    reasoning_enc = tokenizer(example[\"reason\"], truncation=True, max_length=256)\n",
    "\n",
    "    return {\n",
    "        \"input_ids\": input_enc[\"input_ids\"],\n",
    "        \"attention_mask\": input_enc[\"attention_mask\"],\n",
    "        \"reasoning_labels\": reasoning_enc[\"input_ids\"],\n",
    "        \"label_ids\": example[\"label_id\"]\n",
    "    }\n",
    "\n",
    "tokenized = dataset.map(tokenize, batched=False)\n",
    "\n",
    "\n",
    "\n",
    "# 3. Collator\n",
    "\n",
    "def collate_fn(batch):\n",
    "    return {\n",
    "        \"input_ids\": torch.tensor([x[\"input_ids\"] for x in batch]),\n",
    "        \"attention_mask\": torch.tensor([x[\"attention_mask\"] for x in batch]),\n",
    "        \"reasoning_labels\": torch.tensor([x[\"reasoning_labels\"] for x in batch]),\n",
    "        \"label_ids\": torch.tensor([x[\"label_ids\"] for x in batch])\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e5ff0ba",
   "metadata": {},
   "source": [
    "### Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8f87aee",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class ReasoningWithLabelModel(nn.Module):\n",
    "    def __init__(self, model_name, num_labels, loss_weight=1.0):\n",
    "        super().__init__()\n",
    "        # Returns vocab logits by default; ask for hidden states too\n",
    "        self.lm = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "        hidden_size = self.lm.config.hidden_size\n",
    "        self.classifier = nn.Linear(hidden_size, num_labels)\n",
    "        self.loss_weight = float(loss_weight)\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        input_ids=None,\n",
    "        attention_mask=None,\n",
    "        reasoning_labels=None,   # token ids to predict (use -100 for ignore)\n",
    "        label_ids=None,          # class indices (LongTensor) OR\n",
    "        label_onehot=None,       # one-hot / soft labels (FloatTensor)\n",
    "    ):\n",
    "        # === 1) Run the LM to get token-level logits AND hidden states ===\n",
    "        lm_outputs = self.lm(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            output_hidden_states=True,  # so we can pool a representation\n",
    "            use_cache=False             # avoid past_key_values in training\n",
    "        )\n",
    "\n",
    "        # last_hidden_state: (batch, seq_len, hidden)\n",
    "        last_hidden = lm_outputs.hidden_states[-1]\n",
    "        # pool with last token (you may swap for CLS/prefix pooling if desired)\n",
    "        pooled = last_hidden[:, -1, :]                          # (B, H)\n",
    "        logits = self.classifier(pooled)                        # (B, C)\n",
    "\n",
    "        # === 2) Classification loss: L_cls = -∑_i y_i log(ŷ_i) ===\n",
    "        classification_loss = None\n",
    "        if (label_onehot is not None) or (label_ids is not None):\n",
    "            if label_onehot is None:\n",
    "                # convert hard labels -> one-hot\n",
    "                num_labels = logits.size(-1)\n",
    "                label_onehot = F.one_hot(label_ids, num_classes=num_labels).float()\n",
    "            # predicted probabilities\n",
    "            log_probs = F.log_softmax(logits, dim=-1)           # (B, C)\n",
    "            # cross-entropy with one-hot (supports soft labels too)\n",
    "            # reduce over classes, then mean over batch\n",
    "            classification_loss = -(label_onehot * log_probs).sum(dim=-1).mean()\n",
    "\n",
    "        # === 3) Reasoning loss: token-level NLL for seq generation ===\n",
    "        # Shift so that tokens <t predict token t, standard causal LM training\n",
    "        reasoning_loss = None\n",
    "        if reasoning_labels is not None:\n",
    "            # model vocab logits: (B, T, V)\n",
    "            lm_logits = lm_outputs.logits\n",
    "            # shift\n",
    "            shift_logits = lm_logits[:, :-1, :]                 # (B, T-1, V)\n",
    "            shift_labels = reasoning_labels[:, 1:]              # (B, T-1)\n",
    "\n",
    "            # Optional: mask out pads/ignored labels; by convention -100 is ignore_index\n",
    "            ignore_mask = (shift_labels == -100)\n",
    "            # compute log probs\n",
    "            log_probs_t = F.log_softmax(shift_logits, dim=-1)   # (B, T-1, V)\n",
    "\n",
    "            # gather log Pθ(y_t | y_<t, x, i^) at each step t\n",
    "            # For ignore positions, we can safely put any index; we’ll zero them later\n",
    "            safe_labels = torch.where(ignore_mask, torch.zeros_like(shift_labels), shift_labels)\n",
    "            gathered = log_probs_t.gather(dim=-1, index=safe_labels.unsqueeze(-1)).squeeze(-1)  # (B, T-1)\n",
    "\n",
    "            # zero out ignored positions, then sum over time and average over batch\n",
    "            gathered = torch.where(ignore_mask, torch.zeros_like(gathered), gathered)\n",
    "            # NLL is negative sum over time; normalize by number of valid tokens\n",
    "            valid_counts = (~ignore_mask).sum(dim=-1).clamp_min(1)  # (B,)\n",
    "            nll_per_example = -(gathered.sum(dim=-1) / valid_counts)  # (B,)\n",
    "            reasoning_loss = nll_per_example.mean()\n",
    "\n",
    "        # === 4) Total loss: L_intent = L_cls + L_nll ===\n",
    "        total_loss = None\n",
    "        if (classification_loss is not None) and (reasoning_loss is not None):\n",
    "            total_loss = reasoning_loss + self.loss_weight * classification_loss\n",
    "        elif reasoning_loss is not None:\n",
    "            total_loss = reasoning_loss\n",
    "        elif classification_loss is not None:\n",
    "            total_loss = self.loss_weight * classification_loss\n",
    "\n",
    "        return {\n",
    "            \"loss\": total_loss,\n",
    "            \"reasoning_loss\": reasoning_loss,\n",
    "            \"classification_loss\": classification_loss,\n",
    "            \"logits\": logits,\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "194f5585",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 5. Trainer\n",
    "\n",
    "# args = TrainingArguments(\n",
    "#     output_dir=\"./multitask_reasoning\",\n",
    "#     per_device_train_batch_size=2,\n",
    "#     per_device_eval_batch_size=2,\n",
    "#     num_train_epochs=3,\n",
    "#     learning_rate=5e-5,\n",
    "#     logging_steps=20,\n",
    "#     save_strategy=\"epoch\",\n",
    "#     evaluation_strategy=\"epoch\",\n",
    "#     fp16=True,\n",
    "#     report_to=\"tensorboard\"\n",
    "# )\n",
    "\n",
    "\n",
    "training_arguments = TrainingArguments(\n",
    "    output_dir=output_dir,\n",
    "    num_train_epochs=num_train_epochs,\n",
    "    per_device_train_batch_size=per_device_train_batch_size,\n",
    "    gradient_accumulation_steps=gradient_accumulation_steps,\n",
    "    optim=optim,\n",
    "    save_steps=save_steps,\n",
    "    logging_steps=logging_steps,\n",
    "    learning_rate=learning_rate,\n",
    "    weight_decay=weight_decay,\n",
    "    fp16=fp16,\n",
    "    bf16=bf16,\n",
    "    max_grad_norm=max_grad_norm,\n",
    "    max_steps=max_steps,\n",
    "    warmup_ratio=warmup_ratio,\n",
    "    group_by_length=group_by_length,\n",
    "    lr_scheduler_type=lr_scheduler_type,\n",
    "    report_to=\"tensorboard\"\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cea74e3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model = ReasoningWithLabelModel(model_name, num_labels=len(label2id), loss_weight=1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be4efac3",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_arguments,\n",
    "    train_dataset=tokenized[\"train\"],\n",
    "    eval_dataset=tokenized[\"test\"],\n",
    "    data_collator=collate_fn,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39a52cc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*10)\n",
    "print(\"Starting training...\")\n",
    "trainer.train()\n",
    "print(\"Training complete.\")\n",
    "print(\"=\"*10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56549272",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_model = \"/DATA/rohan_kirti/niladri/fine_tune/train_dataset/fine_tune model\"\n",
    "print(\"=\"*10)\n",
    "trainer.model.save_pretrained(new_model)\n",
    "print(\"Model saved\")\n",
    "print(\"=\"*10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
