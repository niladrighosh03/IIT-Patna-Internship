{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6140ba4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    BitsAndBytesConfig,\n",
    "    HfArgumentParser,\n",
    "    TrainingArguments,\n",
    "    pipeline,\n",
    "    logging,\n",
    ")\n",
    "from peft import LoraConfig, PeftModel\n",
    "from trl import SFTTrainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "894e9965",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The model that you want to train from the Hugging Face hub\n",
    "model_name = \"Qwen/Qwen2.5-3B-Instruct\"\n",
    "\n",
    "\n",
    "################################################################################\n",
    "# QLoRA parameters\n",
    "################################################################################\n",
    "\n",
    "# LoRA attention dimension\n",
    "lora_r = 64\n",
    "\n",
    "# Alpha parameter for LoRA scaling\n",
    "lora_alpha = 16\n",
    "\n",
    "# Dropout probability for LoRA layers\n",
    "lora_dropout = 0.1\n",
    "\n",
    "################################################################################\n",
    "# bitsandbytes parameters\n",
    "################################################################################\n",
    "\n",
    "# Activate 4-bit precision base model loading\n",
    "use_4bit = True\n",
    "\n",
    "# Compute dtype for 4-bit base models\n",
    "bnb_4bit_compute_dtype = \"float16\"\n",
    "\n",
    "# Quantization type (fp4 or nf4)\n",
    "bnb_4bit_quant_type = \"nf4\"\n",
    "\n",
    "# Activate nested quantization for 4-bit base models (double quantization)\n",
    "use_nested_quant = False\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "################################################################################\n",
    "# TrainingArguments parameters\n",
    "################################################################################\n",
    "\n",
    "# Output directory where the model predictions and checkpoints will be stored\n",
    "output_dir = \"/DATA/rohan_kirti/niladri/fine_tune/train_dataset/results\"\n",
    "\n",
    "# Number of training epochs\n",
    "num_train_epochs =1\n",
    "\n",
    "# Enable fp16/bf16 training (set bf16 to True with an A100)\n",
    "fp16 = False\n",
    "bf16 = False\n",
    "\n",
    "# Batch size per GPU for training\n",
    "per_device_train_batch_size = 4\n",
    "\n",
    "# Batch size per GPU for evaluation\n",
    "per_device_eval_batch_size = 4\n",
    "\n",
    "# Number of update steps to accumulate the gradients for\n",
    "gradient_accumulation_steps = 1\n",
    "\n",
    "# Enable gradient checkpointing\n",
    "gradient_checkpointing = True\n",
    "\n",
    "# Maximum gradient normal (gradient clipping)\n",
    "max_grad_norm = 0.3\n",
    "\n",
    "# Initial learning rate (AdamW optimizer)\n",
    "learning_rate = 2e-4\n",
    "\n",
    "# Weight decay to apply to all layers except bias/LayerNorm weights\n",
    "weight_decay = 0.001\n",
    "\n",
    "# Optimizer to use\n",
    "optim = \"paged_adamw_32bit\"\n",
    "\n",
    "# Learning rate schedule\n",
    "lr_scheduler_type = \"cosine\"\n",
    "\n",
    "# Number of training steps (overrides num_train_epochs)\n",
    "max_steps = -1\n",
    "\n",
    "# Ratio of steps for a linear warmup (from 0 to learning rate)\n",
    "warmup_ratio = 0.03\n",
    "\n",
    "# Group sequences into batches with same length\n",
    "# Saves memory and speeds up training considerably\n",
    "group_by_length = True\n",
    "\n",
    "# Save checkpoint every X updates steps\n",
    "save_steps = 0\n",
    "\n",
    "# Log every X updates steps\n",
    "logging_steps =2\n",
    "\n",
    "################################################################################\n",
    "# SFT parameters\n",
    "################################################################################\n",
    "\n",
    "# Maximum sequence length to use\n",
    "max_seq_length = None\n",
    "\n",
    "# Pack multiple short examples in the same input sequence to increase efficiency\n",
    "packing = False\n",
    "\n",
    "# Load the entire model on the GPU 0\n",
    "device_map ='auto'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c711e4d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "Your GPU supports bfloat16: accelerate training with bf16=True\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# Load dataset (you can process it here)\n",
    "dataset = load_dataset(\"csv\", data_files='/DATA/rohan_kirti/niladri/fine_tune/train_dataset/dataset.csv', split=\"train\")\n",
    "\n",
    "\n",
    "# # Load tokenizer and model with QLoRA configuration\n",
    "compute_dtype = getattr(torch, bnb_4bit_compute_dtype)\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=use_4bit,\n",
    "    bnb_4bit_quant_type=bnb_4bit_quant_type,\n",
    "    bnb_4bit_compute_dtype=compute_dtype,\n",
    "    bnb_4bit_use_double_quant=use_nested_quant,\n",
    ")\n",
    "\n",
    "# Check GPU compatibility with bfloat16\n",
    "if compute_dtype == torch.float16 and use_4bit:\n",
    "    major, _ = torch.cuda.get_device_capability()\n",
    "    if major >= 8:\n",
    "        print(\"=\" * 80)\n",
    "        print(\"Your GPU supports bfloat16: accelerate training with bf16=True\")\n",
    "        print(\"=\" * 80)\n",
    "\n",
    "\n",
    "# Load LoRA configuration\n",
    "peft_config = LoraConfig(\n",
    "    lora_alpha=lora_alpha,\n",
    "    lora_dropout=lora_dropout,\n",
    "    r=lora_r,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0d748957",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:02<00:00,  1.26s/it]\n"
     ]
    }
   ],
   "source": [
    "# Load base model\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    quantization_config=bnb_config,\n",
    "    # load_in_8bit=True,\n",
    "    device_map='auto'\n",
    ")\n",
    "model.config.use_cache = False\n",
    "model.config.pretraining_tp = 1\n",
    "\n",
    "# Load LLaMA tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"right\" # Fix weird overflow issue with fp16 training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "685ad252",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
     ]
    }
   ],
   "source": [
    "# Set training parameters\n",
    "training_arguments = TrainingArguments(\n",
    "    output_dir=output_dir,\n",
    "    num_train_epochs=num_train_epochs,\n",
    "    per_device_train_batch_size=per_device_train_batch_size,\n",
    "    gradient_accumulation_steps=gradient_accumulation_steps,\n",
    "    optim=optim,\n",
    "    save_steps=save_steps,\n",
    "    logging_steps=logging_steps,\n",
    "    learning_rate=learning_rate,\n",
    "    weight_decay=weight_decay,\n",
    "    fp16=fp16,\n",
    "    bf16=bf16,\n",
    "    max_grad_norm=max_grad_norm,\n",
    "    max_steps=max_steps,\n",
    "    warmup_ratio=warmup_ratio,\n",
    "    group_by_length=group_by_length,\n",
    "    lr_scheduler_type=lr_scheduler_type,\n",
    "    report_to=\"tensorboard\"\n",
    ")\n",
    "\n",
    "# Set supervised fine-tuning parameters\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    train_dataset=dataset,\n",
    "    peft_config=peft_config,\n",
    "    args=training_arguments,\n",
    "    # packing=packing,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31fc88ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*10)\n",
    "print(\"Starting training...\")\n",
    "trainer.train()\n",
    "print(\"Training complete.\")\n",
    "print(\"=\"*10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9437bc26",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_model = \"/DATA/rohan_kirti/niladri/fine_tune/train_dataset/fine_tune model\"\n",
    "print(\"=\"*10)\n",
    "trainer.model.save_pretrained(new_model)\n",
    "print(\"Model saved\")\n",
    "print(\"=\"*10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4fd1a88",
   "metadata": {},
   "source": [
    "## Saved Fine Tune model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "37b7b710",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/DATA/rohan_kirti/miniconda3/envs/nilenv/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:04<00:00,  2.21s/it]\n"
     ]
    }
   ],
   "source": [
    "model_name = \"Qwen/Qwen2.5-3B-Instruct\"\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import torch\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    # low_cpu_mem_usage=True,\n",
    "    # return_dict=True,\n",
    "    # torch_dtype=torch.float16,\n",
    "    device_map='auto',\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2d2cd5c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"right\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ee18b1c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/DATA/rohan_kirti/miniconda3/envs/nilenv/lib/python3.11/site-packages/transformers/generation/utils.py:2479: UserWarning: You are calling .generate() with the `input_ids` being on a device type different than your model's device. `input_ids` is on cpu, whereas the model is on cuda. You may experience unexpected behaviors or slower generation. Please make sure that you have put `input_ids` to the correct device by calling for example input_ids = input_ids.to('cuda') before running `.generate()`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Do you have any advice or resources I should check out? Getting motor insurance for your electric vehicle (EV) like the 2024 Tesla Model 3 is a bit different from traditional gasoline vehicles. Here are some key points and resources to consider:\n",
      "\n",
      "### Key Considerations:\n",
      "1. **Insurance Types**:\n",
      "   - **Collision Coverage**: Covers damages to your car in an accident.\n",
      "   - **Comprehensive Coverage**: Covers damage not caused by collisions, such as theft or vandalism.\n",
      "   - **Liability Insurance**: Covers damages to other people's property and injuries they may suffer due to your fault.\n",
      "   \n",
      "2. **Coverage Levels**:\n",
      "   - **Minimum Required**: In most states, liability coverage is required. However, it might\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import torch\n",
    "input_text = \"Hi, I'm looking to get motor insurance for my new electric vehicle. It's a 2024 Tesla Model 3.\"\n",
    "inputs = tokenizer(input_text, return_tensors=\"pt\")\n",
    "input_len = inputs.input_ids.shape[1]\n",
    "\n",
    "with torch.no_grad():\n",
    "    outputs = base_model.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens=150,\n",
    "        do_sample=True,\n",
    "        temperature=0.8,\n",
    "        top_k=50,\n",
    "        top_p=0.95,\n",
    "        eos_token_id=tokenizer.eos_token_id\n",
    "    )\n",
    "\n",
    "# print(tokenizer.decode(outputs[0], skip_special_tokens=True))\n",
    "\n",
    "generated_tokens = outputs[0, input_len:]\n",
    "full_response = tokenizer.decode(generated_tokens, skip_special_tokens=True)\n",
    "\n",
    "print(full_response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "83487b59",
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft iPeftModelmport LoraConfig, \n",
    "new_model = \"/DATA/rohan_kirti/niladri/fine_tune/train_dataset/fine_tune model_train\"\n",
    "\n",
    "model = PeftModel.from_pretrained(base_model, new_model)\n",
    "model = model.merge_and_unload()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2895392f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reload tokenizer to save it\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"right\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a9f9c391",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('/DATA/rohan_kirti/niladri/fine_tune/train_dataset/qween_fine_tune/tokenizer_config.json',\n",
       " '/DATA/rohan_kirti/niladri/fine_tune/train_dataset/qween_fine_tune/special_tokens_map.json',\n",
       " '/DATA/rohan_kirti/niladri/fine_tune/train_dataset/qween_fine_tune/chat_template.jinja',\n",
       " '/DATA/rohan_kirti/niladri/fine_tune/train_dataset/qween_fine_tune/vocab.json',\n",
       " '/DATA/rohan_kirti/niladri/fine_tune/train_dataset/qween_fine_tune/merges.txt',\n",
       " '/DATA/rohan_kirti/niladri/fine_tune/train_dataset/qween_fine_tune/added_tokens.json',\n",
       " '/DATA/rohan_kirti/niladri/fine_tune/train_dataset/qween_fine_tune/tokenizer.json')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define where to save\n",
    "save_directory = \"/DATA/rohan_kirti/niladri/fine_tune/train_dataset/qween_fine_tune\"\n",
    "\n",
    "# Save the merged model (now a plain Hugging Face model)\n",
    "model.save_pretrained(save_directory)\n",
    "\n",
    "# Save the tokenizer\n",
    "tokenizer.save_pretrained(save_directory)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c2de8f53",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/DATA/rohan_kirti/miniconda3/envs/nilenv/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Loading checkpoint shards: 100%|██████████| 3/3 [00:03<00:00,  1.23s/it]\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import torch\n",
    "save_directory = \"/DATA/rohan_kirti/niladri/fine_tune/train_dataset/qween_fine_tune\"\n",
    "# Load model in FP16 or BF16 or FP32, depending on your hardware\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    save_directory,\n",
    "    device_map=\"auto\"               # Automatically assigns model to available GPU/CPU\n",
    ")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(save_directory, trust_remote_code=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e8f9b8be",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b3b756ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/DATA/rohan_kirti/miniconda3/envs/nilenv/lib/python3.11/site-packages/transformers/generation/utils.py:2479: UserWarning: You are calling .generate() with the `input_ids` being on a device type different than your model's device. `input_ids` is on cpu, whereas the model is on cuda. You may experience unexpected behaviors or slower generation. Please make sure that you have put `input_ids` to the correct device by calling for example input_ids = input_ids.to('cuda') before running `.generate()`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " What kind of coverage options do you recommend?\n",
      "As an AI, I don't have personal experiences or preferences, but I can certainly help you find the best motor insurance options for your 2024 Tesla Model 3. Electric vehicles like yours come with unique features and requirements, so it's essential to choose a policy that caters specifically to them. Here are some recommended coverage options:\n",
      "\n",
      "1. Comprehensive Coverage: This option covers damage to your Tesla Model 3 from accidents, theft, or natural disasters. It's particularly important for electric vehicles since they have specific components like the battery.\n",
      "\n",
      "2. Battery Protection: As an electric vehicle, your battery is a significant investment. Comprehensive Battery Protection is a must-have coverage option to ensure your battery is\n"
     ]
    }
   ],
   "source": [
    "input_text = \"Hi, I'm looking to get motor insurance for my new electric vehicle. It's a 2024 Tesla Model 3.\"\n",
    "inputs = tokenizer(input_text, return_tensors=\"pt\")\n",
    "input_len = inputs.input_ids.shape[1]\n",
    "\n",
    "with torch.no_grad():\n",
    "    outputs = model.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens=150,\n",
    "        do_sample=True,\n",
    "        temperature=0.8,\n",
    "        top_k=50,\n",
    "        top_p=0.95,\n",
    "        eos_token_id=tokenizer.eos_token_id\n",
    "    )\n",
    "\n",
    "# print(tokenizer.decode(outputs[0], skip_special_tokens=True))\n",
    "\n",
    "generated_tokens = outputs[0, input_len:]\n",
    "full_response = tokenizer.decode(generated_tokens, skip_special_tokens=True)\n",
    "\n",
    "print(full_response)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
