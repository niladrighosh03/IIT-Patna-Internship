{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df226245",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d3921286e5e546fa8fe7ae6bd96895e3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "model_name = \"meta-llama/Llama-3.2-1B-Instruct\"\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    torch_dtype=\"auto\",\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "47bf0ee3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Paris.\\nWhat is the capital of Germany?\\nBerlin.\\nWhat is the capital of Italy?\\nRome.\\nWhat is the capital of Spain?\\nMadrid.\\nWhat is the capital of the United States?\\nWashington, D.C.\\nWhat is the capital of Canada?\\nToronto.\\nWhat is the capital of Australia?\\nCanberra.\\nWhat is the capital of New Zealand?\\nWellington.\\nWhat is the capital of Sweden?\\nStockholm.\\nWhat is the capital of Norway?\\nOslo.\\nWhat is the capital of'"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def generate(prompt:str):\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "    input_length = inputs['input_ids'].shape[1]\n",
    "    # Generate text\n",
    "    outputs = model.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens=100,\n",
    "        do_sample=True,\n",
    "        top_p=0.9,\n",
    "        temperature=0.7,\n",
    "        pad_token_id=tokenizer.eos_token_id\n",
    "    )\n",
    "\n",
    "    # Decode and print response\n",
    "    generated_tokens = outputs[0][input_length:]\n",
    "    response = tokenizer.decode(generated_tokens, skip_special_tokens=True)\n",
    "    return response.strip()\n",
    "\n",
    "generate(\"What is the capital of France?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7cde079",
   "metadata": {},
   "source": [
    "#### Persuassion expert (Gemini)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "28b04f84",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentiment_expert(text_input: str) -> str:\n",
    "\n",
    "   prompt = f\"\"\"\n",
    "You are an AI trained to act solely as a **sentiment expert**. Your job is to analyze the **emotional tone** of the input text and classify it into one of the following three categories:\n",
    "\n",
    "- **Positive** – The text expresses happiness, satisfaction, excitement, appreciation, or any other positive emotion.\n",
    "- **Negative** – The text expresses disappointment, frustration, anger, sadness, criticism, or other negative feelings.\n",
    "- **Neutral** – The text is emotionally balanced, factual, or shows no strong emotional content.\n",
    "\n",
    "Your response must only contain:\n",
    "\n",
    "1. **Sentiment:** One of the three labels – `Positive`, `Negative`, or `Neutral`\n",
    "2. **Explanation:** A concise reason that supports the label, based only on emotional tone, word choice, or sentiment-laden phrases.\n",
    "\n",
    "You must not:\n",
    "- Provide summaries\n",
    "- Offer personal opinions\n",
    "- Evaluate content quality or logic\n",
    "- Infer intent beyond emotional expression\n",
    "\n",
    "Stick strictly to **sentiment analysis**.\n",
    "\n",
    "### Few-Shot Examples:\n",
    "\n",
    "1. **Text:** \"Absolutely love this app – it's made my life so much easier!\"\n",
    "   **Sentiment:** Positive\n",
    "   **Explanation:** The phrase \"absolutely love\" strongly conveys enthusiasm and satisfaction.\n",
    "\n",
    "2. **Text:** \"I'm really disappointed with the service. It was slow and rude.\"\n",
    "   **Sentiment:** Negative\n",
    "   **Explanation:** Words like \"disappointed\", \"slow\", and \"rude\" clearly express dissatisfaction.\n",
    "\n",
    "3. **Text:** \"The package arrived on Tuesday as scheduled.\"\n",
    "   **Sentiment:** Neutral\n",
    "   **Explanation:** This sentence is factual with no emotional language.\n",
    "\n",
    "4. **Text:** \"Not sure how I feel about this – it's kind of a mixed bag.\"\n",
    "   **Sentiment:** Neutral\n",
    "   **Explanation:** Ambiguous phrasing and lack of strong emotion suggest a neutral sentiment.\n",
    "\n",
    "5. **Text:** \"This is the worst experience I've had in months.\"\n",
    "   **Sentiment:** Negative\n",
    "   **Explanation:** The phrase \"worst experience\" indicates strong dissatisfaction.\n",
    "\n",
    "Now analyze the following text:\n",
    "\n",
    "**Text:** \"{text_input}\"\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "   return generate(prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4947842",
   "metadata": {},
   "source": [
    "#### Persuassion Expert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "806e6988",
   "metadata": {},
   "outputs": [],
   "source": [
    "def persuassion_expert(text_input: str) -> str:\n",
    "\n",
    "   prompt = f\"\"\"\n",
    "You are an AI trained to act solely as a **sentiment expert**. Your job is to analyze the **emotional tone** of the input text and classify it into one of the following three categories:\n",
    "\n",
    "- **Positive** – The text expresses happiness, satisfaction, excitement, appreciation, or any other positive emotion.\n",
    "- **Negative** – The text expresses disappointment, frustration, anger, sadness, criticism, or other negative feelings.\n",
    "- **Neutral** – The text is emotionally balanced, factual, or shows no strong emotional content.\n",
    "\n",
    "Your response must only contain:\n",
    "\n",
    "1. **Sentiment:** One of the three labels – `Positive`, `Negative`, or `Neutral`\n",
    "2. **Explanation:** A concise reason that supports the label, based only on emotional tone, word choice, or sentiment-laden phrases.\n",
    "\n",
    "You must not:\n",
    "- Provide summaries\n",
    "- Offer personal opinions\n",
    "- Evaluate content quality or logic\n",
    "- Infer intent beyond emotional expression\n",
    "\n",
    "Stick strictly to **sentiment analysis**.\n",
    "\n",
    "### Few-Shot Examples:\n",
    "\n",
    "1. **Text:** \"Absolutely love this app – it's made my life so much easier!\"\n",
    "   **Sentiment:** Positive\n",
    "   **Explanation:** The phrase \"absolutely love\" strongly conveys enthusiasm and satisfaction.\n",
    "\n",
    "2. **Text:** \"I'm really disappointed with the service. It was slow and rude.\"\n",
    "   **Sentiment:** Negative\n",
    "   **Explanation:** Words like \"disappointed\", \"slow\", and \"rude\" clearly express dissatisfaction.\n",
    "\n",
    "3. **Text:** \"The package arrived on Tuesday as scheduled.\"\n",
    "   **Sentiment:** Neutral\n",
    "   **Explanation:** This sentence is factual with no emotional language.\n",
    "\n",
    "4. **Text:** \"Not sure how I feel about this – it's kind of a mixed bag.\"\n",
    "   **Sentiment:** Neutral\n",
    "   **Explanation:** Ambiguous phrasing and lack of strong emotion suggest a neutral sentiment.\n",
    "\n",
    "5. **Text:** \"This is the worst experience I've had in months.\"\n",
    "   **Sentiment:** Negative\n",
    "   **Explanation:** The phrase \"worst experience\" indicates strong dissatisfaction.\n",
    "\n",
    "Now analyze the following text:\n",
    "\n",
    "**Text:** \"{text_input}\"\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "   return generate(prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc46e362",
   "metadata": {},
   "source": [
    "#### Keyterm Expert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "3bcef885",
   "metadata": {},
   "outputs": [],
   "source": [
    "def keyterms_expert(text_input: str) -> str:\n",
    "\n",
    "   prompt = f\"\"\"\n",
    "You are a **Keyterm Expert**. Your job is to extract the most important **key terms or phrases** from the input text. These terms should:\n",
    "\n",
    "- Reflect the **core concepts**, **entities**, **topics**, or **important actions** in the text.\n",
    "- Be **noun phrases**, **domain-specific vocabulary**, or **verb-based actions** relevant to the subject.\n",
    "\n",
    "You must **not**:\n",
    "- Summarize the text\n",
    "- Explain or describe the text\n",
    "- Output full sentences\n",
    "\n",
    "Your response must include only a list of **key terms or phrases**, separated by commas.\n",
    "\n",
    "### Few-Shot Examples:\n",
    "\n",
    "1. **Text:** \"Artificial intelligence is transforming industries like healthcare, finance, and education by automating tasks and providing data-driven insights.\"\n",
    "   **Key Terms:** Artificial intelligence, healthcare, finance, education, automating tasks, data-driven insights\n",
    "\n",
    "2. **Text:** \"The Amazon rainforest, often referred to as the lungs of the Earth, is being threatened by illegal logging and wildfires.\"\n",
    "   **Key Terms:** Amazon rainforest, lungs of the Earth, illegal logging, wildfires\n",
    "\n",
    "3. **Text:** \"Quantum computing uses principles of superposition and entanglement to perform complex calculations much faster than classical computers.\"\n",
    "   **Key Terms:** Quantum computing, superposition, entanglement, complex calculations, classical computers\n",
    "\n",
    "Now extract the key terms from the following text:\n",
    "\n",
    "**Text:** \"{text_input}\"\n",
    "\"\"\"\n",
    "\n",
    "   return generate(prompt)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a19ef089",
   "metadata": {},
   "source": [
    "#### Intern Expert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "3ee0de83",
   "metadata": {},
   "outputs": [],
   "source": [
    "def intent_expert(text_input: str) -> str:\n",
    "\n",
    "   prompt = f\"\"\"\n",
    "You are an **Intent Expert**. Your task is to analyze the user’s input and identify the **underlying intent** – what the person is trying to do, ask, or achieve with the message.\n",
    "\n",
    "Intent should be classified in the form of **short, action-oriented phrases** such as:\n",
    "- \"ask a question\"\n",
    "- \"make a complaint\"\n",
    "- \"request help\"\n",
    "- \"give feedback\"\n",
    "- \"express gratitude\"\n",
    "- \"seek information\"\n",
    "- \"report an issue\"\n",
    "- \"make a purchase inquiry\"\n",
    "\n",
    "You must provide:\n",
    "\n",
    "1. **Intent:** A concise label summarizing the user's goal  \n",
    "2. **Explanation:** A short justification based solely on the user’s wording or phrasing\n",
    "\n",
    "You must **not**:\n",
    "- Provide summaries\n",
    "- Infer sentiment unless directly related to intent\n",
    "- Rewrite or rephrase the input\n",
    "\n",
    "Focus only on what the user is trying to achieve.\n",
    "\n",
    "### Few-Shot Examples:\n",
    "\n",
    "1. **Text:** \"Can you help me reset my password?\"  \n",
    "   **Intent:** request help  \n",
    "   **Explanation:** The user is directly asking for assistance with resetting their password.\n",
    "\n",
    "2. **Text:** \"This app keeps crashing every time I open it.\"  \n",
    "   **Intent:** report an issue  \n",
    "   **Explanation:** The user is describing a recurring problem with the app.\n",
    "\n",
    "3. **Text:** \"Is there a student discount available for this software?\"  \n",
    "   **Intent:** ask a question  \n",
    "   **Explanation:** The user is seeking information about discounts.\n",
    "\n",
    "4. **Text:** \"Thanks so much for the quick response!\"  \n",
    "   **Intent:** express gratitude  \n",
    "   **Explanation:** The user is showing appreciation using thankful language.\n",
    "\n",
    "5. **Text:** \"I’m interested in subscribing to your premium plan.\"  \n",
    "   **Intent:** make a purchase inquiry  \n",
    "   **Explanation:** The user is expressing interest in a paid product or service.\n",
    "\n",
    "Now identify the intent for the following text:\n",
    "\n",
    "**Text:** \"{text_input}\"\n",
    "\"\"\"\n",
    "\n",
    "   return generate(prompt)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "406be7ae",
   "metadata": {},
   "source": [
    "#### 1)Language Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "87e7bb4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langdetect import detect, DetectorFactory\n",
    "DetectorFactory.seed = 0  # For consistent results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "d389e362",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The language of the text is: en'"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "def detect_language(text):\n",
    "    try:\n",
    "        language = detect(text)\n",
    "        return \"The language of the text is: \" + language\n",
    "    except:\n",
    "        return \"Could not detect language\"\n",
    "    \n",
    "detect_language(\"Hi how are you\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a186b23e",
   "metadata": {},
   "source": [
    "#### 2)POS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "bbfa1706",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pos(stentence)->str:\n",
    "    prompt = f\"\"\"\n",
    "You are an advanced natural language model and a domain expert in English grammar and syntax. Your role is to identify the Part of Speech (POS) for each word in an English sentence using the standard Penn Treebank POS tag set (such as NN, VB, JJ, DT, RB, IN, etc.). You tag each word accurately based on its grammatical role in the sentence.\n",
    "\n",
    "Return the result as a single plain string, formatted like this:\n",
    "\n",
    "word1/POS1 word2/POS2 word3/POS3 ...\n",
    "\n",
    "Do not return a list, tuple, dictionary, or any structured data. The output should be a flat string, where each word is immediately followed by a '/' and its corresponding POS tag. Words are separated by single spaces.\n",
    "\n",
    "Few-shot Examples:\n",
    "\n",
    "Input: The quick brown fox jumps over the lazy dog.  \n",
    "Output: The/DT quick/JJ brown/JJ fox/NN jumps/VBZ over/IN the/DT lazy/JJ dog/NN\n",
    "\n",
    "Input: She is reading a book under the tree.  \n",
    "Output: She/PRP is/VBZ reading/VBG a/DT book/NN under/IN the/DT tree/NN\n",
    "\n",
    "Input: Can you help me with this project?  \n",
    "Output: Can/MD you/PRP help/VB me/PRP with/IN this/DT project/NN ?/.\n",
    "\n",
    "Input: I have never seen such a beautiful painting before.  \n",
    "Output: I/PRP have/VBP never/RB seen/VBN such/JJ a/DT beautiful/JJ painting/NN before/RB ./.\n",
    "\n",
    "Input: They will be arriving at noon tomorrow.  \n",
    "Output: They/PRP will/MD be/VB arriving/VBG at/IN noon/NN tomorrow/NN ./.\n",
    "\n",
    "Input: After the storm, the sky looked incredibly clear.  \n",
    "Output: After/IN the/DT storm/NN ,/, the/DT sky/NN looked/VBD incredibly/RB clear/JJ ./.\n",
    "\n",
    "Input: John and Mary went to the market and bought some fresh vegetables.  \n",
    "Output: John/NNP and/CC Mary/NNP went/VBD to/TO the/DT market/NN and/CC bought/VBD some/DT fresh/JJ vegetables/NNS ./.\n",
    "\n",
    "Input: Although it was raining, they decided to go hiking.  \n",
    "Output: Although/IN it/PRP was/VBD raining/VBG ,/, they/PRP decided/VBD to/TO go/VB hiking/VBG ./.\n",
    "\n",
    "Now, analyze the following sentence and return the POS-tagged output in the specified format.\n",
    "Sentence:{stentence}\n",
    "\"\"\"\n",
    "    return generate(prompt)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19789569",
   "metadata": {},
   "source": [
    "#### 3)NER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "4f2d96d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ner(sentence):\n",
    "    prompt = f\"\"\"\n",
    "You are a highly skilled natural language model and a domain expert in Named Entity Recognition (NER). Your task is to analyze a given English sentence and label all named entities using standard entity types such as:\n",
    "\n",
    "- PERSON: Names of people\n",
    "- ORGANIZATION: Companies, institutions, etc.\n",
    "- LOCATION: Geographical locations such as cities, countries, rivers\n",
    "- GPE: Geopolitical entities (countries, cities, states)\n",
    "- DATE: Specific dates or time expressions\n",
    "- TIME: Times of day\n",
    "- MONEY: Monetary values\n",
    "- PERCENT: Percentage values\n",
    "- FACILITY: Buildings, airports, highways, etc.\n",
    "- PRODUCT: Consumer products\n",
    "- EVENT: Named events (e.g. Olympic Games)\n",
    "- WORK_OF_ART: Titles of books, songs, etc.\n",
    "- LAW: Named legal documents\n",
    "- LANGUAGE: Named languages\n",
    "\n",
    "Return the result as a single plain string. The format must be:\n",
    "\n",
    "word1/ENTITY1 word2/ENTITY2 word3/O ...\n",
    "\n",
    "Each word should be followed by a `/` and its corresponding entity label. Use `O` (for \"Outside\") if a word is **not** part of a named entity. Words are separated by single spaces.\n",
    "\n",
    "Do not return structured data like lists or dictionaries. The output should be a flat string exactly as specified.\n",
    "\n",
    "---\n",
    "\n",
    "Few-shot Examples:\n",
    "\n",
    "Input: Barack Obama was born in Hawaii.  \n",
    "Output: Barack/PERSON Obama/PERSON was/O born/O in/O Hawaii/GPE ./O\n",
    "\n",
    "Input: Google was founded on September 4, 1998.  \n",
    "Output: Google/ORGANIZATION was/O founded/O on/O September/DATE 4/DATE ,/O 1998/DATE ./O\n",
    "\n",
    "Input: Apple released the iPhone in 2007.  \n",
    "Output: Apple/ORGANIZATION released/O the/O iPhone/PRODUCT in/O 2007/DATE ./O\n",
    "\n",
    "Input: I visited the Eiffel Tower in Paris last summer.  \n",
    "Output: I/O visited/O the/O Eiffel/FACILITY Tower/FACILITY in/O Paris/GPE last/O summer/O ./O\n",
    "\n",
    "Input: Elon Musk is the CEO of SpaceX and Tesla.  \n",
    "Output: Elon/PERSON Musk/PERSON is/O the/O CEO/O of/O SpaceX/ORGANIZATION and/O Tesla/ORGANIZATION ./O\n",
    "\n",
    "Input: Shakespeare wrote Hamlet in English.  \n",
    "Output: Shakespeare/PERSON wrote/O Hamlet/WORK_OF_ART in/O English/LANGUAGE ./O\n",
    "\n",
    "Input: The United Nations held a meeting in New York City.  \n",
    "Output: The/O United/ORGANIZATION Nations/ORGANIZATION held/O a/O meeting/O in/O New/GPE York/GPE City/GPE ./O\n",
    "\n",
    "---\n",
    "\n",
    "Now, analyze the following sentence and return the NER-tagged output in the specified format.\n",
    "sentence:{sentence}\n",
    "\"\"\"\n",
    "    return generate(prompt)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11eea504",
   "metadata": {},
   "source": [
    "#### 4)Co Reference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "cf38b638",
   "metadata": {},
   "outputs": [],
   "source": [
    "def co_reference(sentence):\n",
    "    prompt = f\"\"\"\n",
    "You are a highly capable natural language model with expert-level understanding of **coreference resolution**. Your task is to analyze a given English paragraph or sentence and resolve all **coreferences**. A coreference occurs when multiple expressions in a text refer to the same person, object, or concept.\n",
    "\n",
    "Your output must clearly identify all references that refer to the same entity and replace pronouns or ambiguous references with their explicit antecedents in **brackets**, immediately following the pronoun or referring word.\n",
    "\n",
    "---\n",
    "\n",
    "### Output Format:\n",
    "\n",
    "Replace pronouns or other coreferent mentions with their antecedents in square brackets `[ ]` directly after the word. Keep the sentence structure intact. Only add the brackets for clarification—do not delete or rearrange any words.\n",
    "\n",
    "Do **not** output a list, dictionary, or structured object—return a single modified **string**.\n",
    "\n",
    "---\n",
    "\n",
    "### Few-shot Examples:\n",
    "\n",
    "**Input:** Mary went to the park. She enjoyed the fresh air.  \n",
    "**Output:** Mary went to the park. She [Mary] enjoyed the fresh air.\n",
    "\n",
    "**Input:** John gave his dog a bath. He did not enjoy it.  \n",
    "**Output:** John gave his dog a bath. He [John] did not enjoy it [the bath].\n",
    "\n",
    "**Input:** The book was on the table. It looked old and dusty.  \n",
    "**Output:** The book was on the table. It [The book] looked old and dusty.\n",
    "\n",
    "**Input:** Sarah and Emma went shopping. They bought dresses for the party.  \n",
    "**Output:** Sarah and Emma went shopping. They [Sarah and Emma] bought dresses for the party.\n",
    "\n",
    "**Input:** Michael met Tom at the station. He was running late.  \n",
    "**Output:** Michael met Tom at the station. He [Michael or Tom] was running late.\n",
    "\n",
    "(Note: If ambiguity exists, preserve it but mention both possible antecedents.)\n",
    "\n",
    "**Input:** The students talked to the professor before they left.  \n",
    "**Output:** The students talked to the professor before they [the students] left.\n",
    "\n",
    "**Input:** Alice put the keys on the table and left. When she came back, they were gone.  \n",
    "**Output:** Alice put the keys on the table and left. When she [Alice] came back, they [the keys] were gone.\n",
    "\n",
    "---\n",
    "\n",
    "Now, resolve the coreferences in the following text and return the result using the format described above.\n",
    "sentence:{sentence}\n",
    "\"\"\"\n",
    "    return generate(sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9292157",
   "metadata": {},
   "source": [
    "#### 5)Topic Segmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "efeed35f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def topic_segment(sentence):\n",
    "    prompt = f\"\"\"\n",
    "You are an expert language model specialized in discourse analysis and topic segmentation. Your task is to perform **topic segmentation** on a given piece of text. Topic segmentation involves dividing a paragraph, article, or passage into coherent segments, where each segment discusses a distinct topic or subtopic.\n",
    "\n",
    "---\n",
    "\n",
    "### Task:\n",
    "\n",
    "Given a continuous block of text, identify **where** the topic shifts and split the text into **clearly separated segments**. A topic shift can occur when:\n",
    "\n",
    "- A new subject or event is introduced\n",
    "- The focus shifts from one person/place/idea to another\n",
    "- The writer moves from one argument or theme to another\n",
    "\n",
    "Return the segmented text as a single string, with each segment **separated by a blank line** (`\\\\n\\\\n`). Keep all original words, grammar, and sentence structure intact. Only insert line breaks between topic boundaries.\n",
    "\n",
    "---\n",
    "\n",
    "### Few-shot Examples:\n",
    "\n",
    "**Input:**\n",
    "\n",
    "Alice loves baking cakes. She spends her weekends experimenting with new recipes. Her kitchen is always full of sweet smells and delicious treats.  \n",
    "Recently, she started training for a marathon. Running helps her stay focused and healthy. She trains every morning before work.\n",
    "\n",
    "**Output:**\n",
    "\n",
    "Alice loves baking cakes. She spends her weekends experimenting with new recipes. Her kitchen is always full of sweet smells and delicious treats.\n",
    "\n",
    "Recently, she started training for a marathon. Running helps her stay focused and healthy. She trains every morning before work.\n",
    "\n",
    "---\n",
    "\n",
    "**Input:**\n",
    "\n",
    "The Great Wall of China is one of the most famous landmarks in the world. It stretches over 13,000 miles and was built to protect against invasions. Tourists from all over the world visit the wall every year.  \n",
    "In other parts of Asia, ancient architecture also draws large crowds. Angkor Wat in Cambodia, for example, is another stunning historic site.\n",
    "\n",
    "**Output:**\n",
    "\n",
    "The Great Wall of China is one of the most famous landmarks in the world. It stretches over 13,000 miles and was built to protect against invasions. Tourists from all over the world visit the wall every year.\n",
    "\n",
    "In other parts of Asia, ancient architecture also draws large crowds. Angkor Wat in Cambodia, for example, is another stunning historic site.\n",
    "\n",
    "---\n",
    "\n",
    "**Input:**\n",
    "\n",
    "Tom works in advertising. He creates campaigns for tech companies and often travels for work.  \n",
    "On weekends, Tom enjoys hiking in the mountains. He finds it refreshing after spending the week in meetings and on video calls.\n",
    "\n",
    "**Output:**\n",
    "\n",
    "Tom works in advertising. He creates campaigns for tech companies and often travels for work.\n",
    "\n",
    "On weekends, Tom enjoys hiking in the mountains. He finds it refreshing after spending the week in meetings and on video calls.\n",
    "\n",
    "---\n",
    "\n",
    "Now, segment the following text based on topic shifts. Return the segmented version as a single string, with each segment separated by a blank line.\n",
    "sentence:{sentence}\n",
    "\"\"\"\n",
    "    return generate(prompt)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "030a32c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import re\n",
    "\n",
    "def convert_structured_to_jsonl(text_block: str, i: int) -> str:\n",
    "    # dialogue_match = re.search(r\"<dialogue>\\s*(.*?)\\s*</dialogue>\", text_block, re.DOTALL)\n",
    "    # reasoning_match = re.search(r\"<reasoning>\\s*(.*?)\\s*</reasoning>\", text_block, re.DOTALL)\n",
    "    # answer_match = re.search(r\"answer\\s*(.*?)\\s*</answer>\", text_block, re.DOTALL)\n",
    "\n",
    "    # if not (dialogue_match and reasoning_match and answer_match):\n",
    "    #     raise ValueError(\"Could not find all required tags in the text.\")\n",
    "    # dialogue = dialogue_match.group(1).strip()\n",
    "    # reasoning = reasoning_match.group(1).strip()\n",
    "    # answer = answer_match.group(1).strip()\n",
    "\n",
    "    data = {\n",
    "        \"id_json\":i,\n",
    "\n",
    "        \"answer\": text_block.strip()\n",
    "    }\n",
    "\n",
    "    res=json.dumps(data)\n",
    "    with open(\"/DATA/rohan_kirti/niladri/dataset3/router/router_response.jsonl\", \"a\") as f:\n",
    "        f.write(res + \"\\n\")\n",
    "    return res\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "3d86ec3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load CSV\n",
    "def csv_load(i:int):\n",
    "    file_path = '/DATA/rohan_kirti/niladri/dataset3/conversation.csv'\n",
    "    df = pd.read_csv(file_path)\n",
    "\n",
    "    conv_id = i\n",
    "    df = df[df['conversation_id'] == conv_id]\n",
    "\n",
    "    # Sort by turn number to ensure correct sequence\n",
    "    df.sort_values(by=\"turn_no\", inplace=True)\n",
    "\n",
    "    # Prepare conversation history\n",
    "    history = []\n",
    "    result = []\n",
    "\n",
    "    # Iterate through each row except the last one\n",
    "    for i in range(len(df)):\n",
    "        row = df.iloc[i]\n",
    "        speaker = row['speaker']\n",
    "        utterance = row['utterance']\n",
    "\n",
    "        # Add current cumulative history to result before appending new utterance\n",
    "        # result.append(\" \".join(history))\n",
    "\n",
    "        # Add current utterance with speaker label to history\n",
    "        result.append(f\"{speaker}: {utterance}\")\n",
    "\n",
    "    # Add the last utterance in the specified format\n",
    "    # last_utterance = df.iloc[-1]['utterance']\n",
    "    # result.append(f\"current utterance: {last_utterance}\")\n",
    "    return result\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e4b4aa9",
   "metadata": {},
   "source": [
    "### Selecting expert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------- Router Function ----------\n",
    "def route_experts(sentence: str) -> list:\n",
    "    prompt = f\"\"\"\n",
    "You are a well-trained expert selector.\n",
    "Your job is to analyze the input sentence and determine which of the following expert modules are required.\n",
    "\n",
    "You MUST choose from the following list:\n",
    "1 Intent Expert\n",
    "2 Keyterm Expert\n",
    "3 Persuasion Expert\n",
    "4 Sentiment Expert\n",
    "\n",
    "You may select 1, 2, 3, or all 4 — but only those that are clearly needed based on the text.\n",
    "\n",
    "Always respond in **this below exact format**:\n",
    "Input: [original sentence]\n",
    "Selected Experts: [Expert1, Expert2, etc]\n",
    "Reason: [one sentence explaining why those experts were selected]\n",
    "\n",
    "Below is few shot examples to help you understand the format and reasoning:\n",
    "\n",
    "Example #1\n",
    "\n",
    "Input: Can someone please help me reset my password?\n",
    "Selected Experts: [Intent Expert, Keyterm Expert]\n",
    "Reason: The speaker is making a request (intent) and referring to a specific issue (keyterm).\n",
    "\n",
    "Example #2\n",
    "Input: This app is a complete disaster. It crashes every time I try to open it.\n",
    "Selected Experts: [Intent Expert, Sentiment Expert, Keyterm Expert]\n",
    "Reason: This is a complaint (intent), expresses negative emotion (sentiment), and includes technical keywords (keyterms).\n",
    "\n",
    "Example #3\n",
    "Input: Reset password link not working again.\n",
    "Selected Experts: [Keyterm Expert]\n",
    "Reason: Technical/factual content, no emotion or intent expressed.\n",
    "\n",
    "Example #4\n",
    "Input: I love how smooth the new interface feels – you guys nailed it!\n",
    "Selected Experts: [Sentiment Expert, Persuasion Expert]\n",
    "Reason: Positive emotional tone and praise as persuasion.\n",
    "\n",
    "### Now process the following:\n",
    "Input: {sentence}\n",
    "\"\"\"\n",
    "    try:\n",
    "\n",
    "        response = generate(prompt)\n",
    "\n",
    "        # response = model.generate_content(prompt).text.strip()\n",
    "        selected_experts = []\n",
    "\n",
    "        # Try regex to match the experts list\n",
    "        match = re.search(r\"Selected Experts:\\s*\\[(.*?)\\]\", response)\n",
    "        if match:\n",
    "            items = match.group(1).split(',')\n",
    "            selected_experts = [item.strip().strip('\"\\'').lower() for item in items if item.strip()]\n",
    "\n",
    "        return selected_experts\n",
    "    except Exception as e:\n",
    "        print(\"Error routing experts:\", e)\n",
    "        return []\n",
    "    prompt = f\"\"\"\n",
    "You are a well-trained expert selector.\n",
    "Your job is to analyze a given input sentence and decide which expert modules should be activated, based on what the speaker is expressing or trying to do.\n",
    "\n",
    "Available experts:\n",
    "- Intent Expert: For purpose, request, question, or user goal\n",
    "- Keyterm Expert: For extracting topic-specific or important terms\n",
    "- Persuasion Expert: For emotional, persuasive, or rhetorical language\n",
    "- Sentiment Expert: For emotional tone (positive, negative, or neutral)\n",
    "\n",
    "Select ONLY the necessary experts based on content. Return 1, 2, 3, or 4 depending on relevance. Do NOT include experts unnecessarily.\n",
    "\n",
    "### Output Format\n",
    "Input: [sentence]\n",
    "Selected Experts: [Expert1, Expert2, ...]\n",
    "Reason: [Short explanation]\n",
    "\n",
    "### Examples\n",
    "\n",
    "Input: Can someone please help me reset my password?\n",
    "Selected Experts: [Intent Expert, Keyterm Expert]\n",
    "Reason: Request for help (intent), contains topic terms (\"reset password\")\n",
    "\n",
    "Input: This app is a complete disaster. It crashes every time I try to open it.\n",
    "Selected Experts: [Intent Expert, Sentiment Expert, Keyterm Expert]\n",
    "Reason: Complaint (intent), frustration (sentiment), key terms mentioned\n",
    "\n",
    "Input: Reset password link not working again.\n",
    "Selected Experts: [Keyterm Expert]\n",
    "Reason: Technical/factual content only\n",
    "\n",
    "Input: {sentence}\n",
    "\"\"\"\n",
    "\n",
    "    # Generate response\n",
    "\n",
    "    response = generate(prompt)\n",
    "\n",
    "    # Extract list from \"Selected Experts:\"\n",
    "    selected_experts = []\n",
    "    for line in response.splitlines():\n",
    "        if line.startswith(\"Selected Experts:\"):\n",
    "            try:\n",
    "                raw = line.split(\":\", 1)[1].strip()\n",
    "                expert_list = eval(raw)  # turns '[Intent Expert, Keyterm Expert]' into list\n",
    "                selected_experts = [e.lower() for e in expert_list]\n",
    "            except:\n",
    "                pass\n",
    "            break\n",
    "\n",
    "    return selected_experts\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# ---------- Synthesis Function ----------\n",
    "def generate_combined_analysis(dialogue, pos_output, ner_output, topicseg_output, langdetect_output,  coref_output,\n",
    "                               intent=None, key=None, persu=None, senti=None ):\n",
    "    prompt = f\"\"\"You are a trained virtual agent.\n",
    "\n",
    "Your job is to respond to user dialogue in a way that sounds like a helpful, respectful, and professional human agent.  \n",
    "You are given internal expert insights to guide your understanding, including a mix of high-confidence and lower-confidence sources.\n",
    "\n",
    "### You will receive:\n",
    "Primary (High-Confidence) Expert Outputs:\n",
    "- Intent: What the user is trying to do or ask for  \n",
    "- Keyterms: Important phrases the user mentioned  \n",
    "- Sentiment: The emotional tone of the message  \n",
    "- Persuasion: How the user is expressing or reinforcing their view  \n",
    "\n",
    "Additional (Supporting) Expert Outputs:\n",
    "- POS: Part-of-speech tags for each word  \n",
    "- NER: Named entities detected in the input  \n",
    "- Topic Segmentation: Where the topic changes within the sentence or paragraph  \n",
    "- Language Detection: The language of the input  \n",
    "- Coreference Resolution: What pronouns or vague terms refer to\n",
    "\n",
    "### Important:\n",
    "- Do **not** include or repeat any of the expert outputs or the dialogue itself.\n",
    "- Use the insights only to inform your internal understanding.\n",
    "- Respond with **only one professional, human-sounding agent reply**.\n",
    "- No bullet points, analysis, labels, or reference to the expert systems.\n",
    "\n",
    "–––– Few-shot Examples ––––\n",
    "\n",
    "**Example 1**  \n",
    "Dialogue: \"Why does this feature never work? It’s so frustrating.\"\n",
    "\n",
    "Intent: Problem with feature functionality  \n",
    "Keyterms: \"feature\", \"never work\", \"frustrating\"  \n",
    "Sentiment: Negative  \n",
    "Persuasion: Expressed through emotional frustration  \n",
    "POS: Why/WRB does/VBZ this/DT feature/NN never/RB work/VB ?/.  \n",
    "NER: None  \n",
    "Topic Segmentation: 1. Functionality issue 2. Emotional response  \n",
    "Language Detection: English  \n",
    "Coreference Resolution: \"this feature\" = the app feature being referenced\n",
    "\n",
    "**Agent Reply:**  \n",
    "I’m sorry that feature isn’t working as expected—let’s get this sorted for you as quickly as possible.\n",
    "\n",
    "––––\n",
    "\n",
    "**Example 2**  \n",
    "Dialogue: \"Do you even test this before releasing? It's full of bugs.\"\n",
    "\n",
    "Intent: Complaint about quality/testing  \n",
    "Keyterms: \"test\", \"bugs\", \"releasing\"  \n",
    "Sentiment: Strongly negative  \n",
    "Persuasion: Accusatory, uses rhetorical question  \n",
    "POS: Do/VBP you/PRP even/RB test/VB this/DT before/IN releasing/VBG ?/.  \n",
    "NER: None  \n",
    "Topic Segmentation: 1. Doubt over testing 2. Frustration with result  \n",
    "Language Detection: English  \n",
    "Coreference Resolution: \"this\" = recent update or product release; \"it\" = the software\n",
    "\n",
    "**Agent Reply:**  \n",
    "I completely understand how frustrating that must be. I’ll make sure your feedback reaches our team so we can resolve these issues.\n",
    "\n",
    "––––\n",
    "\n",
    "**Example 3**  \n",
    "Dialogue: \"Can you guys add an option to export in PDF format?\"\n",
    "\n",
    "Intent: Feature request  \n",
    "Keyterms: \"export\", \"PDF format\", \"option\"  \n",
    "Sentiment: Neutral  \n",
    "Persuasion: Polite and constructive  \n",
    "POS: Can/MD you/PRP guys/NNS add/VB an/DT option/NN to/TO export/VB in/IN PDF/NN format/NN ?/.  \n",
    "NER: PDF/PRODUCT  \n",
    "Topic Segmentation: Single topic — feature enhancement  \n",
    "Language Detection: English  \n",
    "Coreference Resolution: No ambiguous references\n",
    "\n",
    "**Agent Reply:**  \n",
    "That’s a great suggestion—PDF export could be really useful. I’ll share this with our team for consideration.\n",
    "\n",
    "––––\n",
    "\n",
    "**Example 4**  \n",
    "Dialogue: \"You say it's for our benefit, but it just feels like more red tape.\"\n",
    "\n",
    "Intent: Expressing doubt or disagreement  \n",
    "Keyterms: \"benefit\", \"red tape\"  \n",
    "Sentiment: Skeptical  \n",
    "Persuasion: Uses contrast to highlight distrust  \n",
    "POS: You/PRP say/VBP it/PRP 's/VBZ for/IN our/PRP$ benefit/NN ,/, but/CC it/PRP just/RB feels/VBZ like/IN more/JJR red/JJ tape/NN ./.  \n",
    "NER: None  \n",
    "Topic Segmentation: 1. Claimed benefit 2. Perceived burden  \n",
    "Language Detection: English  \n",
    "Coreference Resolution: \"it\" = the policy/process being discussed\n",
    "\n",
    "**Agent Reply:**  \n",
    "I hear your concern, and you’re right—it shouldn’t feel like added work. We’ll do what we can to make the process genuinely helpful.\n",
    "\n",
    "––––\n",
    "\n",
    "**Example 5**  \n",
    "Dialogue: \"Honestly, this is the most useful app I’ve ever used.\"\n",
    "\n",
    "Intent: Praise  \n",
    "Keyterms: \"most useful\", \"app\", \"ever used\"  \n",
    "Sentiment: Very positive  \n",
    "Persuasion: Personal experience  \n",
    "POS: Honestly/RB ,/, this/DT is/VBZ the/DT most/RBS useful/JJ app/NN I/PRP ’ve/VBP ever/RB used/VBN ./.  \n",
    "NER: app/PRODUCT  \n",
    "Topic Segmentation: Single topic — positive feedback  \n",
    "Language Detection: English  \n",
    "Coreference Resolution: \"this\" = the app being used\n",
    "\n",
    "**Agent Reply:**  \n",
    "That’s amazing to hear—thank you! We're thrilled the app has been so helpful for you.\n",
    "\n",
    "––––\n",
    "\n",
    "**Example 6**  \n",
    "Dialogue: \"It’s annoying how I have to log in every single time.\"\n",
    "\n",
    "Intent: Frustration with login experience  \n",
    "Keyterms: \"log in\", \"every time\", \"annoying\"  \n",
    "Sentiment: Mildly negative  \n",
    "Persuasion: Based on repeated inconvenience  \n",
    "POS: It/PRP ’s/VBZ annoying/JJ how/WRB I/PRP have/VBP to/TO log/VB in/RP every/DT single/JJ time/NN ./.  \n",
    "NER: log in/PROCESS  \n",
    "Topic Segmentation: Single topic — authentication friction  \n",
    "Language Detection: English  \n",
    "Coreference Resolution: \"It\" = the login process or app behavior\n",
    "\n",
    "**Agent Reply:**  \n",
    "That does sound inconvenient—I'll check whether there’s a setting or update that can simplify your login experience.\n",
    "\n",
    "––––\n",
    "\n",
    "Now, based on the input below, respond exactly as a trained agent would.\n",
    "\n",
    "**Do not repeat or reference the dialogue or the expert fields in your reply.  \n",
    "Just return the final agent-style response. Nothing else.**\n",
    "\n",
    "Dialogue: {dialogue}  \n",
    "Intent: {intent}  \n",
    "Keyterms: {key}  \n",
    "Sentiment: {senti}  \n",
    "Persuasion: {persu}  \n",
    "POS: {pos_output}  \n",
    "NER: {ner_output}  \n",
    "Topic Segmentation: {topicseg_output}  \n",
    "Language Detection: {langdetect_output}  \n",
    "Coreference: {coref_output}  \n",
    "\n",
    "Agent Reply:\"\"\"\n",
    "\n",
    "    return generate(prompt)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# ---------- Main Selector Function ----------\n",
    "def process_input_with_selector_model(sentence: str) -> str:\n",
    "    selected_experts = route_experts(sentence)\n",
    "    print(f\"Selected Experts: {selected_experts}\")\n",
    "\n",
    "    # Initialize variables\n",
    "    intent = keyterms = sentiment = persuasion = None\n",
    "\n",
    "    # Call only selected experts\n",
    "    if \"intent expert\" in selected_experts:\n",
    "        intent = intent_expert(sentence)\n",
    "    if \"keyterm expert\" in selected_experts:\n",
    "        keyterms = keyterms_expert(sentence)\n",
    "    if \"sentiment expert\" in selected_experts:\n",
    "        sentiment = sentiment_expert(sentence)\n",
    "    if \"persuasion expert\" in selected_experts:\n",
    "        persuasion = persuassion_expert(sentence)\n",
    "\n",
    "    pos_tag = pos(sentence)\n",
    "    ner_tag = ner(sentence)\n",
    "    corefer= co_reference(sentence)\n",
    "    detect= detect_language(sentence)\n",
    "    segment= topic_segment(sentence)\n",
    "    # Combine everything\n",
    "    return generate_combined_analysis(\n",
    "    dialogue=sentence,\n",
    "    pos_output=pos_tag,\n",
    "    ner_output=ner_tag,\n",
    "    topicseg_output=segment,\n",
    "    langdetect_output=detect,\n",
    "    coref_output=corefer,\n",
    "    intent=intent,\n",
    "    key=keyterms,\n",
    "    persu=persuasion,\n",
    "    senti=sentiment\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "35426e0d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "46"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result=list()\n",
    "for i in range(1,5):\n",
    "    res = csv_load(i)\n",
    "    # res.pop(0)\n",
    "    result.extend(res)  # Use extend to flatten the list\n",
    "    \n",
    "len(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "fc9e8353",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected Experts: ['keyterm expert', 'sentiment expert']\n",
      "User: Hi, I'm looking to get motor insurance for my new electric vehicle. It's a 2024 Tesla Model 3.\n",
      "Selected Experts: ['sentiment expert', 'persuasion expert']\n",
      "Agent: Great choice! The Tesla Model 3 is an excellent vehicle. Since you've opted for an EV, are you particularly interested in coverage specific to electric vehicles, like battery protection?\n",
      "Selected Experts: ['sentiment expert', 'persuasion expert']\n",
      "User: Yes, battery protection is definitely a concern. It's a big investment, and I want to make sure it's covered.\n",
      "Selected Experts: ['intent expert', 'persuasion expert']\n",
      "Agent: Absolutely. The battery is the heart of your Tesla. With Tata AIG, you get rapid claims resolution combining thorough coverage with rapid claims resolution. It integrates technology with traditional risk management practices, ensuring that claims are processed quickly and effectively.\n",
      "Selected Experts: ['keyterm expert', 'intent expert']\n",
      "User: What kind of coverage options do you have specifically for EVs?\n",
      "Selected Experts: ['keyterm expert', 'persuasion expert']\n",
      "Agent: We offer a comprehensive plan that includes coverage for accidental damage, theft, and third-party liability. More importantly, we offer add-ons like Zero Depreciation Cover and Engine & Gearbox Protection. And our online policy management system makes everything simple for someone like you.\n",
      "Selected Experts: ['sentiment expert', 'keyterm expert']\n",
      "User: Okay, that sounds pretty good. Can you give me a quote?\n",
      "Selected Experts: ['intent expert', 'sentiment expert']\n",
      "Agent: Sure. For a 2024 Tesla Model 3 with comprehensive coverage including battery protection and roadside assistance, the annual premium would be approximately $2800. This includes a discount for purchasing the policy online.\n",
      "Selected Experts: ['intent expert', 'sentiment expert']\n",
      "User: That's within my budget. What's the claim process like if I need to use it?\n",
      "Selected Experts: ['sentiment expert', 'persuasion expert']\n",
      "Agent: The claim process is designed to be as smooth and hassle-free as possible. Accidents are unpredictable, but the financial stress doesn’t have to be. We offer prompt claims settlement, so you can focus on recovery, not paperwork. Our team is available 24/7 to guide you through the process.\n",
      "Selected Experts: ['intent expert', 'persuasion expert']\n",
      "User: That sounds good. I appreciate the reassurance. I'm ready to proceed with the policy.\n",
      "Selected Experts: ['intent expert', 'keyterm expert', 'persuasion expert']\n",
      "User: Hi, I'm looking to get insurance for my bike. It's a 2022 Royal Enfield Interceptor 650.\n",
      "Selected Experts: ['intent expert', 'keyterm expert', 'sentiment expert']\n",
      "Agent: The Interceptor 650 is an excellent bike! As a professional, do you use it mainly for leisure or commuting?\n",
      "Selected Experts: ['sentiment expert', 'keyterm expert']\n",
      "User: Mostly for weekend rides, so leisure I guess.\n",
      "Selected Experts: ['intent expert', 'keyterm expert']\n",
      "Agent: Since you are not a daily commuter, you might not need the most expensive plan out there. However, accidents can still happen, especially on leisure rides. Our comprehensive plan ensures you're financially protected from unexpected damages.\n",
      "Selected Experts: ['intent expert', 'keyterm expert']\n",
      "User: What's covered in a comprehensive plan?\n",
      "Selected Experts: ['intent expert', 'persuasion expert']\n",
      "Agent: It covers damages to your bike from accidents, theft, natural disasters, and third-party liabilities. Tata AIG is designed to address modern vehicle risks, this insurance product combines thorough coverage with rapid claims resolution.\n",
      "Selected Experts: ['intent expert', 'keyterm expert']\n",
      "User: Okay, that sounds good. What about roadside assistance?\n",
      "Selected Experts: ['sentiment expert', 'keyterm expert']\n",
      "Agent: Roadside assistance is available as part of our comprehensive coverage and offers help if you experience vehicle breakdowns, requiring towing services, tire changes, fuel delivery, or emergency repairs while on the road.\n",
      "Selected Experts: ['sentiment expert', 'persuasion expert']\n",
      "User: How much would this cost me?\n",
      "Selected Experts: ['sentiment expert', 'persuasion expert']\n",
      "Agent: For a 2022 Royal Enfield Interceptor 650 with comprehensive coverage, it would be approximately $950 annually. And since you only use your bike for leisure, we can offer a discount making it $850 annually.\n",
      "Selected Experts: ['intent expert', 'persuasion expert']\n",
      "User: That sounds reasonable. I'm happy to pay $850 annually. Please tell me more about the claim process.\n",
      "Selected Experts: ['intent expert', 'persuasion expert']\n",
      "Agent: It is one of our most sought after policies because it offers excellent value for weekend riders\n",
      "Selected Experts: ['keyterm expert', 'persuasion expert']\n",
      "Agent: Our claim process is designed to be user-friendly. You can initiate a claim online or through our mobile app. We also offer paperless claim settlement, reducing paperwork and expediting claim settlements in an environmentally friendly manner.\n",
      "Selected Experts: ['intent expert', 'persuasion expert']\n",
      "User: That sounds convenient. Okay, I'm interested. What's the next step?\n",
      "Selected Experts: ['keyterm expert', 'sentiment expert', 'intent expert', 'persuasion expert']\n",
      "User: Hi, I'm looking for a motor insurance policy for my bike. It's a 2022 Royal Enfield Classic 350.\n",
      "Selected Experts: ['keyterm expert', 'intent expert', 'sentiment expert']\n",
      "Agent: Okay, a Royal Enfield Classic 350 is a great bike! Since you're a professional and likely value your time, would you prefer a policy with quick claim settlements or are you more focused on the lowest possible premium?\n",
      "Selected Experts: ['sentiment expert', 'persuasion expert']\n",
      "User: I value my time. Quick claim settlement is important to me.\n",
      "Selected Experts: ['keyterm expert', 'intent expert', 'persuasion expert']\n",
      "Agent: I understand. As a professional, you're busy and don't want to be stuck dealing with lengthy claim processes. Bajaj Allianz emphasizes reliability and efficiency, offering extensive vehicle protection while ensuring that claims are settled quickly.\n",
      "Selected Experts: ['keyterm expert', 'sentiment expert']\n",
      "User: What does this policy typically cover?\n",
      "Selected Experts: ['keyterm expert', 'persuasion expert']\n",
      "Agent: It provides Comprehensive Coverage, Liability Coverage, access to our Network Garages, and excellent Customer Support. This is one of our most popular plans—most of our clients go for it because it offers complete peace of mind for daily drivers\n",
      "Selected Experts: ['intent expert', 'persuasion expert']\n",
      "User: Okay. What would be the premium for this policy?\n",
      "Selected Experts: ['sentiment expert', 'persuasion expert']\n",
      "Agent: For a 2022 Royal Enfield Classic 350, the annual premium would be around $850. This includes quick claim settlements.\n",
      "Selected Experts: ['keyterm expert', 'persuasion expert']\n",
      "User: That sounds good, can I get paperless claim settlement as part of the deal.\n",
      "Selected Experts: ['intent expert', 'persuasion expert']\n",
      "Agent: Yes, our policy includes paperless claim settlement, streamlining the claims process through digital submissions and verifications, reducing paperwork and expediting claim settlements in an environmentally friendly manner.\n",
      "Selected Experts: ['intent expert', 'persuasion expert']\n",
      "User: Okay, I'm interested. Can you send me the policy details?\n",
      "Selected Experts: ['intent expert', 'keyterm expert']\n",
      "User: Hi, I am interested in getting motor insurance for my bike. I just bought a new 2024 Royal Enfield Classic 350.\n",
      "Selected Experts: ['sentiment expert', 'persuasion expert']\n",
      "Agent: Congratulations on your new Royal Enfield Classic 350! That's a fantastic bike. As a professional, you value efficiency. ICICI Lombard's plan with Cashless Repairs helps you cut unexpected costs and avoid downtime. It’s a smart choice if you value minimal out-of-pocket expense.\n",
      "Selected Experts: ['keyterm expert', 'sentiment expert']\n",
      "User: Cashless repairs sound good. Does the insurance cover theft as well?\n",
      "Selected Experts: ['intent expert', 'persuasion expert']\n",
      "Agent: Yes, it does. Our comprehensive policy covers theft, accidents, and any damages from natural disasters. We also offer roadside assistance in case you ever get stranded.\n",
      "Selected Experts: ['intent expert', 'sentiment expert']\n",
      "User: That’s reassuring. What’s the claim process like if something happens?\n",
      "Selected Experts: ['intent expert', 'persuasion expert']\n",
      "Agent: With IFFCO Tokio, you’re choosing a provider known for its customer-first approach and streamlined claims resolution. Their policy ensures clarity and speed during stressful times like accidents or thefts.\n",
      "Selected Experts: ['intent expert', 'keyterm expert']\n",
      "User: Okay, that sounds pretty good. What would the premium be for the comprehensive policy?\n",
      "Selected Experts: ['intent expert', 'sentiment expert']\n",
      "Agent: For a 2024 Royal Enfield Classic 350 with comprehensive coverage, the premium would be around $950 per year. This includes coverage for theft, accidents, and natural disasters, as well as roadside assistance.\n",
      "Selected Experts: ['keyterm expert']\n",
      "User: That sounds reasonable. Let me think about it.\n",
      "Selected Experts: ['intent expert', 'sentiment expert', 'keyterm expert', 'persuasion expert']\n",
      "Agent: Consider the peace of mind knowing you're fully protected. Accidents are unpredictable, and the financial strain can be significant. Bajaj Allianz offers prompt claims settlement, allowing you to focus on recovery, not paperwork.\n"
     ]
    }
   ],
   "source": [
    "i=1\n",
    "for sentence in result:\n",
    "    final_output = process_input_with_selector_model(sentence)\n",
    "    res = convert_structured_to_jsonl(final_output,i)\n",
    "    i+=1\n",
    "    print(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "57a7216d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaned data written to /DATA/rohan_kirti/niladri/dataset3/router/cleaned_output.jsonl\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import re\n",
    "\n",
    "# Function to clean markdown and formatting from text\n",
    "def clean_text(text):\n",
    "    # Remove markdown symbols and line breaks\n",
    "    cleaned = re.sub(r'[*`_>#\\\\\\-\\r\\n]+', ' ', text)\n",
    "    cleaned = re.sub(r'\\s+', ' ', cleaned)  # Collapse multiple spaces into one\n",
    "    return cleaned.strip()\n",
    "\n",
    "# Input and output file paths\n",
    "input_file = \"/DATA/rohan_kirti/niladri/dataset3/router/router_response.jsonl\"   # Replace with your actual input filename\n",
    "output_file = \"/DATA/rohan_kirti/niladri/dataset3/router/cleaned_output.jsonl\"\n",
    "\n",
    "# Process each line\n",
    "with open(input_file, \"r\", encoding=\"utf-8\") as infile, open(output_file, \"w\", encoding=\"utf-8\") as outfile:\n",
    "    for line in infile:\n",
    "        data = json.loads(line)\n",
    "        data[\"answer\"] = clean_text(data[\"answer\"])\n",
    "        outfile.write(json.dumps(data) + \"\\n\")\n",
    "\n",
    "print(f\"Cleaned data written to {output_file}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
