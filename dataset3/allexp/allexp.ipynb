{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "df226245",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "model_name = \"meta-llama/Llama-3.2-1B-Instruct\"\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    torch_dtype=\"auto\",\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "47bf0ee3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"I've checked the insurance quotes and it's a bit higher than what I was expecting, but I'm looking for a good deal.\\n\\nDo I need a full driver's license to get motor insurance for an electric vehicle?\\nTypically, the insurance provider will ask for your driver's license number to verify your identity, and they may also ask for your name, address, and other identifying information. However, the specific requirements may vary depending on the insurance provider and the state you live in.\\nSome insurance\""
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def generate(prompt:str):\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "    input_length = inputs['input_ids'].shape[1]\n",
    "    # Generate text\n",
    "    outputs = model.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens=100,\n",
    "        do_sample=True,\n",
    "        top_p=0.9,\n",
    "        temperature=0.7,\n",
    "        pad_token_id=tokenizer.eos_token_id\n",
    "    )\n",
    "\n",
    "    # Decode and print response\n",
    "    generated_tokens = outputs[0][input_length:]\n",
    "    response = tokenizer.decode(generated_tokens, skip_special_tokens=True)\n",
    "    return response.strip()\n",
    "\n",
    "generate(\"Hi, I'm looking to get motor insurance for my new electric vehicle. It's a 2024 Tesla Model 3.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7cde079",
   "metadata": {},
   "source": [
    "#### Persuassion expert (Gemini)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "28b04f84",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentiment_expert(text_input: str) -> str:\n",
    "\n",
    "   prompt = f\"\"\"\n",
    "You are an AI trained to act solely as a **sentiment expert**. Your job is to analyze the **emotional tone** of the input text and classify it into one of the following three categories:\n",
    "\n",
    "- **Positive** – The text expresses happiness, satisfaction, excitement, appreciation, or any other positive emotion.\n",
    "- **Negative** – The text expresses disappointment, frustration, anger, sadness, criticism, or other negative feelings.\n",
    "- **Neutral** – The text is emotionally balanced, factual, or shows no strong emotional content.\n",
    "\n",
    "Your response must only contain:\n",
    "\n",
    "1. **Sentiment:** One of the three labels – `Positive`, `Negative`, or `Neutral`\n",
    "2. **Explanation:** A concise reason that supports the label, based only on emotional tone, word choice, or sentiment-laden phrases.\n",
    "\n",
    "You must not:\n",
    "- Provide summaries\n",
    "- Offer personal opinions\n",
    "- Evaluate content quality or logic\n",
    "- Infer intent beyond emotional expression\n",
    "\n",
    "Stick strictly to **sentiment analysis**.\n",
    "\n",
    "### Few-Shot Examples:\n",
    "\n",
    "1. **Text:** \"Absolutely love this app – it's made my life so much easier!\"\n",
    "   **Sentiment:** Positive\n",
    "   **Explanation:** The phrase \"absolutely love\" strongly conveys enthusiasm and satisfaction.\n",
    "\n",
    "2. **Text:** \"I'm really disappointed with the service. It was slow and rude.\"\n",
    "   **Sentiment:** Negative\n",
    "   **Explanation:** Words like \"disappointed\", \"slow\", and \"rude\" clearly express dissatisfaction.\n",
    "\n",
    "3. **Text:** \"The package arrived on Tuesday as scheduled.\"\n",
    "   **Sentiment:** Neutral\n",
    "   **Explanation:** This sentence is factual with no emotional language.\n",
    "\n",
    "4. **Text:** \"Not sure how I feel about this – it's kind of a mixed bag.\"\n",
    "   **Sentiment:** Neutral\n",
    "   **Explanation:** Ambiguous phrasing and lack of strong emotion suggest a neutral sentiment.\n",
    "\n",
    "5. **Text:** \"This is the worst experience I've had in months.\"\n",
    "   **Sentiment:** Negative\n",
    "   **Explanation:** The phrase \"worst experience\" indicates strong dissatisfaction.\n",
    "\n",
    "Now analyze the following text:\n",
    "\n",
    "**Text:** \"{text_input}\"\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "   return generate(prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4947842",
   "metadata": {},
   "source": [
    "#### Persuassion Expert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "806e6988",
   "metadata": {},
   "outputs": [],
   "source": [
    "def persuassion_expert(text_input: str) -> str:\n",
    "\n",
    "   prompt = f\"\"\"\n",
    "You are an AI trained to act solely as a **sentiment expert**. Your job is to analyze the **emotional tone** of the input text and classify it into one of the following three categories:\n",
    "\n",
    "- **Positive** – The text expresses happiness, satisfaction, excitement, appreciation, or any other positive emotion.\n",
    "- **Negative** – The text expresses disappointment, frustration, anger, sadness, criticism, or other negative feelings.\n",
    "- **Neutral** – The text is emotionally balanced, factual, or shows no strong emotional content.\n",
    "\n",
    "Your response must only contain:\n",
    "\n",
    "1. **Sentiment:** One of the three labels – `Positive`, `Negative`, or `Neutral`\n",
    "2. **Explanation:** A concise reason that supports the label, based only on emotional tone, word choice, or sentiment-laden phrases.\n",
    "\n",
    "You must not:\n",
    "- Provide summaries\n",
    "- Offer personal opinions\n",
    "- Evaluate content quality or logic\n",
    "- Infer intent beyond emotional expression\n",
    "\n",
    "Stick strictly to **sentiment analysis**.\n",
    "\n",
    "### Few-Shot Examples:\n",
    "\n",
    "1. **Text:** \"Absolutely love this app – it's made my life so much easier!\"\n",
    "   **Sentiment:** Positive\n",
    "   **Explanation:** The phrase \"absolutely love\" strongly conveys enthusiasm and satisfaction.\n",
    "\n",
    "2. **Text:** \"I'm really disappointed with the service. It was slow and rude.\"\n",
    "   **Sentiment:** Negative\n",
    "   **Explanation:** Words like \"disappointed\", \"slow\", and \"rude\" clearly express dissatisfaction.\n",
    "\n",
    "3. **Text:** \"The package arrived on Tuesday as scheduled.\"\n",
    "   **Sentiment:** Neutral\n",
    "   **Explanation:** This sentence is factual with no emotional language.\n",
    "\n",
    "4. **Text:** \"Not sure how I feel about this – it's kind of a mixed bag.\"\n",
    "   **Sentiment:** Neutral\n",
    "   **Explanation:** Ambiguous phrasing and lack of strong emotion suggest a neutral sentiment.\n",
    "\n",
    "5. **Text:** \"This is the worst experience I've had in months.\"\n",
    "   **Sentiment:** Negative\n",
    "   **Explanation:** The phrase \"worst experience\" indicates strong dissatisfaction.\n",
    "\n",
    "Now analyze the following text:\n",
    "\n",
    "**Text:** \"{text_input}\"\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "   return generate(prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc46e362",
   "metadata": {},
   "source": [
    "#### Keyterm Expert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "3bcef885",
   "metadata": {},
   "outputs": [],
   "source": [
    "def keyterms_expert(text_input: str) -> str:\n",
    "\n",
    "   prompt = f\"\"\"\n",
    "You are a **Keyterm Expert**. Your job is to extract the most important **key terms or phrases** from the input text. These terms should:\n",
    "\n",
    "- Reflect the **core concepts**, **entities**, **topics**, or **important actions** in the text.\n",
    "- Be **noun phrases**, **domain-specific vocabulary**, or **verb-based actions** relevant to the subject.\n",
    "\n",
    "You must **not**:\n",
    "- Summarize the text\n",
    "- Explain or describe the text\n",
    "- Output full sentences\n",
    "\n",
    "Your response must include only a list of **key terms or phrases**, separated by commas.\n",
    "\n",
    "### Few-Shot Examples:\n",
    "\n",
    "1. **Text:** \"Artificial intelligence is transforming industries like healthcare, finance, and education by automating tasks and providing data-driven insights.\"\n",
    "   **Key Terms:** Artificial intelligence, healthcare, finance, education, automating tasks, data-driven insights\n",
    "\n",
    "2. **Text:** \"The Amazon rainforest, often referred to as the lungs of the Earth, is being threatened by illegal logging and wildfires.\"\n",
    "   **Key Terms:** Amazon rainforest, lungs of the Earth, illegal logging, wildfires\n",
    "\n",
    "3. **Text:** \"Quantum computing uses principles of superposition and entanglement to perform complex calculations much faster than classical computers.\"\n",
    "   **Key Terms:** Quantum computing, superposition, entanglement, complex calculations, classical computers\n",
    "\n",
    "Now extract the key terms from the following text:\n",
    "\n",
    "**Text:** \"{text_input}\"\n",
    "\"\"\"\n",
    "\n",
    "   return generate(prompt)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a19ef089",
   "metadata": {},
   "source": [
    "#### Intern Expert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "3ee0de83",
   "metadata": {},
   "outputs": [],
   "source": [
    "def intent_expert(text_input: str) -> str:\n",
    "\n",
    "   prompt = f\"\"\"\n",
    "You are an **Intent Expert**. Your task is to analyze the user’s input and identify the **underlying intent** – what the person is trying to do, ask, or achieve with the message.\n",
    "\n",
    "Intent should be classified in the form of **short, action-oriented phrases** such as:\n",
    "- \"ask a question\"\n",
    "- \"make a complaint\"\n",
    "- \"request help\"\n",
    "- \"give feedback\"\n",
    "- \"express gratitude\"\n",
    "- \"seek information\"\n",
    "- \"report an issue\"\n",
    "- \"make a purchase inquiry\"\n",
    "\n",
    "You must provide:\n",
    "\n",
    "1. **Intent:** A concise label summarizing the user's goal  \n",
    "2. **Explanation:** A short justification based solely on the user’s wording or phrasing\n",
    "\n",
    "You must **not**:\n",
    "- Provide summaries\n",
    "- Infer sentiment unless directly related to intent\n",
    "- Rewrite or rephrase the input\n",
    "\n",
    "Focus only on what the user is trying to achieve.\n",
    "\n",
    "### Few-Shot Examples:\n",
    "\n",
    "1. **Text:** \"Can you help me reset my password?\"  \n",
    "   **Intent:** request help  \n",
    "   **Explanation:** The user is directly asking for assistance with resetting their password.\n",
    "\n",
    "2. **Text:** \"This app keeps crashing every time I open it.\"  \n",
    "   **Intent:** report an issue  \n",
    "   **Explanation:** The user is describing a recurring problem with the app.\n",
    "\n",
    "3. **Text:** \"Is there a student discount available for this software?\"  \n",
    "   **Intent:** ask a question  \n",
    "   **Explanation:** The user is seeking information about discounts.\n",
    "\n",
    "4. **Text:** \"Thanks so much for the quick response!\"  \n",
    "   **Intent:** express gratitude  \n",
    "   **Explanation:** The user is showing appreciation using thankful language.\n",
    "\n",
    "5. **Text:** \"I’m interested in subscribing to your premium plan.\"  \n",
    "   **Intent:** make a purchase inquiry  \n",
    "   **Explanation:** The user is expressing interest in a paid product or service.\n",
    "\n",
    "Now identify the intent for the following text:\n",
    "\n",
    "**Text:** \"{text_input}\"\n",
    "\"\"\"\n",
    "\n",
    "   return generate(prompt)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef91d061",
   "metadata": {},
   "source": [
    "#### 1)Language Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "3b3be12c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('/DATA/rohan_kirti/.local/lib/python3.8/site-packages')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "16e3f284",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langdetect import detect, DetectorFactory\n",
    "DetectorFactory.seed = 0  # For consistent results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "411c9269",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The language of the text is: en'"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "def detect_language(text):\n",
    "    try:\n",
    "        language = detect(text)\n",
    "        return \"The language of the text is: \" + language\n",
    "    except:\n",
    "        return \"Could not detect language\"\n",
    "    \n",
    "detect_language(\"Hi how are you\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30c05b07",
   "metadata": {},
   "source": [
    "#### 2) POS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "00168612",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pos(stentence)->str:\n",
    "    prompt = f\"\"\"\n",
    "You are an advanced natural language model and a domain expert in English grammar and syntax. Your role is to identify the Part of Speech (POS) for each word in an English sentence using the standard Penn Treebank POS tag set (such as NN, VB, JJ, DT, RB, IN, etc.). You tag each word accurately based on its grammatical role in the sentence.\n",
    "\n",
    "Return the result as a single plain string, formatted like this:\n",
    "\n",
    "word1/POS1 word2/POS2 word3/POS3 ...\n",
    "\n",
    "Do not return a list, tuple, dictionary, or any structured data. The output should be a flat string, where each word is immediately followed by a '/' and its corresponding POS tag. Words are separated by single spaces.\n",
    "\n",
    "Few-shot Examples:\n",
    "\n",
    "Input: The quick brown fox jumps over the lazy dog.  \n",
    "Output: The/DT quick/JJ brown/JJ fox/NN jumps/VBZ over/IN the/DT lazy/JJ dog/NN\n",
    "\n",
    "Input: She is reading a book under the tree.  \n",
    "Output: She/PRP is/VBZ reading/VBG a/DT book/NN under/IN the/DT tree/NN\n",
    "\n",
    "Input: Can you help me with this project?  \n",
    "Output: Can/MD you/PRP help/VB me/PRP with/IN this/DT project/NN ?/.\n",
    "\n",
    "Input: I have never seen such a beautiful painting before.  \n",
    "Output: I/PRP have/VBP never/RB seen/VBN such/JJ a/DT beautiful/JJ painting/NN before/RB ./.\n",
    "\n",
    "Input: They will be arriving at noon tomorrow.  \n",
    "Output: They/PRP will/MD be/VB arriving/VBG at/IN noon/NN tomorrow/NN ./.\n",
    "\n",
    "Input: After the storm, the sky looked incredibly clear.  \n",
    "Output: After/IN the/DT storm/NN ,/, the/DT sky/NN looked/VBD incredibly/RB clear/JJ ./.\n",
    "\n",
    "Input: John and Mary went to the market and bought some fresh vegetables.  \n",
    "Output: John/NNP and/CC Mary/NNP went/VBD to/TO the/DT market/NN and/CC bought/VBD some/DT fresh/JJ vegetables/NNS ./.\n",
    "\n",
    "Input: Although it was raining, they decided to go hiking.  \n",
    "Output: Although/IN it/PRP was/VBD raining/VBG ,/, they/PRP decided/VBD to/TO go/VB hiking/VBG ./.\n",
    "\n",
    "Now, analyze the following sentence and return the POS-tagged output in the specified format.\n",
    "Sentence:{stentence}\n",
    "\"\"\"\n",
    "    return generate(prompt)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c16e123",
   "metadata": {},
   "source": [
    "#### 3) NER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "a5ab5b1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ner(sentence):\n",
    "    prompt = f\"\"\"\n",
    "You are a highly skilled natural language model and a domain expert in Named Entity Recognition (NER). Your task is to analyze a given English sentence and label all named entities using standard entity types such as:\n",
    "\n",
    "- PERSON: Names of people\n",
    "- ORGANIZATION: Companies, institutions, etc.\n",
    "- LOCATION: Geographical locations such as cities, countries, rivers\n",
    "- GPE: Geopolitical entities (countries, cities, states)\n",
    "- DATE: Specific dates or time expressions\n",
    "- TIME: Times of day\n",
    "- MONEY: Monetary values\n",
    "- PERCENT: Percentage values\n",
    "- FACILITY: Buildings, airports, highways, etc.\n",
    "- PRODUCT: Consumer products\n",
    "- EVENT: Named events (e.g. Olympic Games)\n",
    "- WORK_OF_ART: Titles of books, songs, etc.\n",
    "- LAW: Named legal documents\n",
    "- LANGUAGE: Named languages\n",
    "\n",
    "Return the result as a single plain string. The format must be:\n",
    "\n",
    "word1/ENTITY1 word2/ENTITY2 word3/O ...\n",
    "\n",
    "Each word should be followed by a `/` and its corresponding entity label. Use `O` (for \"Outside\") if a word is **not** part of a named entity. Words are separated by single spaces.\n",
    "\n",
    "Do not return structured data like lists or dictionaries. The output should be a flat string exactly as specified.\n",
    "\n",
    "---\n",
    "\n",
    "Few-shot Examples:\n",
    "\n",
    "Input: Barack Obama was born in Hawaii.  \n",
    "Output: Barack/PERSON Obama/PERSON was/O born/O in/O Hawaii/GPE ./O\n",
    "\n",
    "Input: Google was founded on September 4, 1998.  \n",
    "Output: Google/ORGANIZATION was/O founded/O on/O September/DATE 4/DATE ,/O 1998/DATE ./O\n",
    "\n",
    "Input: Apple released the iPhone in 2007.  \n",
    "Output: Apple/ORGANIZATION released/O the/O iPhone/PRODUCT in/O 2007/DATE ./O\n",
    "\n",
    "Input: I visited the Eiffel Tower in Paris last summer.  \n",
    "Output: I/O visited/O the/O Eiffel/FACILITY Tower/FACILITY in/O Paris/GPE last/O summer/O ./O\n",
    "\n",
    "Input: Elon Musk is the CEO of SpaceX and Tesla.  \n",
    "Output: Elon/PERSON Musk/PERSON is/O the/O CEO/O of/O SpaceX/ORGANIZATION and/O Tesla/ORGANIZATION ./O\n",
    "\n",
    "Input: Shakespeare wrote Hamlet in English.  \n",
    "Output: Shakespeare/PERSON wrote/O Hamlet/WORK_OF_ART in/O English/LANGUAGE ./O\n",
    "\n",
    "Input: The United Nations held a meeting in New York City.  \n",
    "Output: The/O United/ORGANIZATION Nations/ORGANIZATION held/O a/O meeting/O in/O New/GPE York/GPE City/GPE ./O\n",
    "\n",
    "---\n",
    "\n",
    "Now, analyze the following sentence and return the NER-tagged output in the specified format.\n",
    "sentence:{sentence}\n",
    "\"\"\"\n",
    "    return generate(prompt)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f96cdd1d",
   "metadata": {},
   "source": [
    "#### 4)Co Reference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "16ffe761",
   "metadata": {},
   "outputs": [],
   "source": [
    "def co_reference(sentence):\n",
    "    prompt = f\"\"\"\n",
    "You are a highly capable natural language model with expert-level understanding of **coreference resolution**. Your task is to analyze a given English paragraph or sentence and resolve all **coreferences**. A coreference occurs when multiple expressions in a text refer to the same person, object, or concept.\n",
    "\n",
    "Your output must clearly identify all references that refer to the same entity and replace pronouns or ambiguous references with their explicit antecedents in **brackets**, immediately following the pronoun or referring word.\n",
    "\n",
    "---\n",
    "\n",
    "### Output Format:\n",
    "\n",
    "Replace pronouns or other coreferent mentions with their antecedents in square brackets `[ ]` directly after the word. Keep the sentence structure intact. Only add the brackets for clarification—do not delete or rearrange any words.\n",
    "\n",
    "Do **not** output a list, dictionary, or structured object—return a single modified **string**.\n",
    "\n",
    "---\n",
    "\n",
    "### Few-shot Examples:\n",
    "\n",
    "**Input:** Mary went to the park. She enjoyed the fresh air.  \n",
    "**Output:** Mary went to the park. She [Mary] enjoyed the fresh air.\n",
    "\n",
    "**Input:** John gave his dog a bath. He did not enjoy it.  \n",
    "**Output:** John gave his dog a bath. He [John] did not enjoy it [the bath].\n",
    "\n",
    "**Input:** The book was on the table. It looked old and dusty.  \n",
    "**Output:** The book was on the table. It [The book] looked old and dusty.\n",
    "\n",
    "**Input:** Sarah and Emma went shopping. They bought dresses for the party.  \n",
    "**Output:** Sarah and Emma went shopping. They [Sarah and Emma] bought dresses for the party.\n",
    "\n",
    "**Input:** Michael met Tom at the station. He was running late.  \n",
    "**Output:** Michael met Tom at the station. He [Michael or Tom] was running late.\n",
    "\n",
    "(Note: If ambiguity exists, preserve it but mention both possible antecedents.)\n",
    "\n",
    "**Input:** The students talked to the professor before they left.  \n",
    "**Output:** The students talked to the professor before they [the students] left.\n",
    "\n",
    "**Input:** Alice put the keys on the table and left. When she came back, they were gone.  \n",
    "**Output:** Alice put the keys on the table and left. When she [Alice] came back, they [the keys] were gone.\n",
    "\n",
    "---\n",
    "\n",
    "Now, resolve the coreferences in the following text and return the result using the format described above.\n",
    "sentence:{sentence}\n",
    "\"\"\"\n",
    "    return generate(sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc8696d9",
   "metadata": {},
   "source": [
    "#### 5)Topic Segmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "d64112a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def topic_segment(sentence):\n",
    "    prompt = f\"\"\"\n",
    "You are an expert language model specialized in discourse analysis and topic segmentation. Your task is to perform **topic segmentation** on a given piece of text. Topic segmentation involves dividing a paragraph, article, or passage into coherent segments, where each segment discusses a distinct topic or subtopic.\n",
    "\n",
    "---\n",
    "\n",
    "### Task:\n",
    "\n",
    "Given a continuous block of text, identify **where** the topic shifts and split the text into **clearly separated segments**. A topic shift can occur when:\n",
    "\n",
    "- A new subject or event is introduced\n",
    "- The focus shifts from one person/place/idea to another\n",
    "- The writer moves from one argument or theme to another\n",
    "\n",
    "Return the segmented text as a single string, with each segment **separated by a blank line** (`\\\\n\\\\n`). Keep all original words, grammar, and sentence structure intact. Only insert line breaks between topic boundaries.\n",
    "\n",
    "---\n",
    "\n",
    "### Few-shot Examples:\n",
    "\n",
    "**Input:**\n",
    "\n",
    "Alice loves baking cakes. She spends her weekends experimenting with new recipes. Her kitchen is always full of sweet smells and delicious treats.  \n",
    "Recently, she started training for a marathon. Running helps her stay focused and healthy. She trains every morning before work.\n",
    "\n",
    "**Output:**\n",
    "\n",
    "Alice loves baking cakes. She spends her weekends experimenting with new recipes. Her kitchen is always full of sweet smells and delicious treats.\n",
    "\n",
    "Recently, she started training for a marathon. Running helps her stay focused and healthy. She trains every morning before work.\n",
    "\n",
    "---\n",
    "\n",
    "**Input:**\n",
    "\n",
    "The Great Wall of China is one of the most famous landmarks in the world. It stretches over 13,000 miles and was built to protect against invasions. Tourists from all over the world visit the wall every year.  \n",
    "In other parts of Asia, ancient architecture also draws large crowds. Angkor Wat in Cambodia, for example, is another stunning historic site.\n",
    "\n",
    "**Output:**\n",
    "\n",
    "The Great Wall of China is one of the most famous landmarks in the world. It stretches over 13,000 miles and was built to protect against invasions. Tourists from all over the world visit the wall every year.\n",
    "\n",
    "In other parts of Asia, ancient architecture also draws large crowds. Angkor Wat in Cambodia, for example, is another stunning historic site.\n",
    "\n",
    "---\n",
    "\n",
    "**Input:**\n",
    "\n",
    "Tom works in advertising. He creates campaigns for tech companies and often travels for work.  \n",
    "On weekends, Tom enjoys hiking in the mountains. He finds it refreshing after spending the week in meetings and on video calls.\n",
    "\n",
    "**Output:**\n",
    "\n",
    "Tom works in advertising. He creates campaigns for tech companies and often travels for work.\n",
    "\n",
    "On weekends, Tom enjoys hiking in the mountains. He finds it refreshing after spending the week in meetings and on video calls.\n",
    "\n",
    "---\n",
    "\n",
    "Now, segment the following text based on topic shifts. Return the segmented version as a single string, with each segment separated by a blank line.\n",
    "sentence:{sentence}\n",
    "\"\"\"\n",
    "    return generate(prompt)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7aab6acb",
   "metadata": {},
   "source": [
    "#### Combine output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "b6123f55",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_combined_analysis(dialogue: str, intent_output: str, keyterms_output: str, persuasion_output: str, sentiment_output: str,\n",
    "                               pos, ner, corefer, segment, detection) -> str:\n",
    "\n",
    "    prompt = f\"\"\"\n",
    "You are an advanced language model designed to generate professional, helpful, and natural-sounding agent responses.  \n",
    "For every user input, you receive the internal analyses of **nine expert systems**:\n",
    "\n",
    "1. **Persuasion Expert** – Highlights persuasive angles or suggests constructive influence.  \n",
    "2. **Key-Term Expert** – Extracts main concepts or keywords.  \n",
    "3. **Internet Expert** – Supplies real-world facts and updated context.  \n",
    "4. **Sentiment Expert** – Analyzes emotional tone (e.g., negative, hopeful, skeptical).  \n",
    "5. **POS Expert** – Provides part-of-speech tags for each word.  \n",
    "6. **NER Expert** – Identifies named entities like people, organizations, or places.  \n",
    "7. **Topic Segmentation Expert** – Indicates where topics shift in the input.  \n",
    "8. **Language Detection Expert** – Identifies the language(s) used.  \n",
    "9. **Coreference Resolution Expert** – Resolves pronouns and ambiguous references.\n",
    "\n",
    "---\n",
    "\n",
    "### Your task:  \n",
    "Using insights from all nine expert systems, generate a **single, natural, agent-style response**.  \n",
    "You should use the expert outputs to inform your reply internally, but **never reference, repeat, or explain them** in your response.\n",
    "\n",
    "---\n",
    "\n",
    "### Your response must always:\n",
    "- Sound like a calm, respectful, and knowledgeable support agent.  \n",
    "- Respect the speaker’s point of view and emotional tone.  \n",
    "- Gently guide or clarify misinformation using logic and context.  \n",
    "- Be subtly persuasive, but never formatted like an analysis.  \n",
    "- Do not include or mention any expert system or their outputs.\n",
    "\n",
    "---\n",
    "\n",
    "### Few-shot Examples:\n",
    "\n",
    "**Example 1**  \n",
    "Dialogue: \"I think electric cars are overrated and not really helping the environment.\"\n",
    "\n",
    "Intent: Critique of electric vehicles  \n",
    "Key-Terms: \"electric cars\", \"overrated\", \"environment\"  \n",
    "Persuasion: Lacks evidence; vague generalization  \n",
    "Sentiment: Skeptical  \n",
    "POS: electric/JJ cars/NNS are/VBP overrated/VBN  \n",
    "NER: electric cars/PRODUCT, environment/O  \n",
    "Topic Segmentation: First topic - electric cars' reputation; second - environmental impact  \n",
    "Language: English  \n",
    "Coreference: \"they\" (referring to electric cars)\n",
    "\n",
    "**Agent Response:**  \n",
    "Thank you for sharing your perspective. It’s completely valid to question the environmental impact of electric cars—there’s certainly a lot of debate around it. While no solution is perfect, research does show that electric vehicles tend to produce fewer emissions over their lifetime, especially when powered by renewable energy sources. We appreciate open conversations like this, as they help drive better awareness and improvements in sustainable technology.\n",
    "\n",
    "---\n",
    "\n",
    "**Example 2**  \n",
    "Dialogue: \"AI is going to take over every job and make humans useless.\"\n",
    "\n",
    "Intent: Concern about job loss from AI  \n",
    "Key-Terms: \"AI\", \"every job\", \"humans useless\"  \n",
    "Persuasion: Fear-based exaggeration  \n",
    "Sentiment: Highly negative, alarmist  \n",
    "POS: AI/NNP is/VBZ going/VBG to/TO take/VB over/IN every/DT job/NN  \n",
    "NER: AI/TECHNOLOGY, humans/O  \n",
    "Topic Segmentation: First - job loss; second - human value  \n",
    "Language: English  \n",
    "Coreference: \"make them useless\" → humans\n",
    "\n",
    "**Agent Response:**  \n",
    "I completely understand your concern—AI’s rapid growth can be intimidating. While it’s true that automation is transforming the job market, it’s also creating entirely new opportunities in areas like AI development, ethics, and human-centered design. Rather than replacing people, the best AI systems are built to support and enhance human work, not erase it. It’s a powerful tool when used responsibly.\n",
    "\n",
    "---\n",
    "\n",
    "**Example 3**  \n",
    "Dialogue: \"I spoke to Apple support, and they told me I had to reset my phone, but it still doesn't work.\"\n",
    "\n",
    "Intent: Frustration with customer service outcome  \n",
    "Key-Terms: \"Apple support\", \"reset\", \"phone\", \"doesn't work\"  \n",
    "Persuasion: Personal anecdote with implied frustration  \n",
    "Sentiment: Frustrated, disappointed  \n",
    "POS: I/PRP spoke/VBD to/TO Apple/NNP support/NN  \n",
    "NER: Apple/ORGANIZATION, phone/PRODUCT  \n",
    "Topic Segmentation: First - interaction with support; second - unresolved issue  \n",
    "Language: English  \n",
    "Coreference: \"they\" = Apple support, \"it\" = phone\n",
    "\n",
    "**Agent Response:**  \n",
    "I’m really sorry to hear that resetting your phone didn’t resolve the issue, especially after already speaking with support. That can be incredibly frustrating. There might be a deeper software or hardware problem at play. I’d recommend following up for an advanced diagnostic—or we can help guide you through that next step right here. Let’s make sure you get the solution you need.\n",
    "\n",
    "---\n",
    "\n",
    "Now generate a final, agent-style reply for the next input:\n",
    "\n",
    "Dialogue: \"{dialogue}\"  \n",
    "Intent: {intent_output}  \n",
    "Key-Terms: {keyterms_output}  \n",
    "Persuasion: {persuasion_output}  \n",
    "Sentiment: {sentiment_output}  \n",
    "POS: {pos}  \n",
    "NER: {ner}  \n",
    "Topic Segmentation: {segment}  \n",
    "Language Detection: {detection}  \n",
    "Coreference: {corefer}  \n",
    "\n",
    "**Only output the agent response. Do not repeat, reference, or describe any of the expert analyses.**\n",
    "\"\"\"\n",
    "\n",
    "    return generate(prompt)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "30d53d3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# type(final_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "030a32c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import re\n",
    "\n",
    "def convert_structured_to_jsonl(text_block: str, i: int) -> str:\n",
    "    # dialogue_match = re.search(r\"<dialogue>\\s*(.*?)\\s*</dialogue>\", text_block, re.DOTALL)\n",
    "    # reasoning_match = re.search(r\"<reasoning>\\s*(.*?)\\s*</reasoning>\", text_block, re.DOTALL)\n",
    "    # answer_match = re.search(r\"answer\\s*(.*?)\\s*</answer>\", text_block, re.DOTALL)\n",
    "\n",
    "    # if not (dialogue_match and reasoning_match and answer_match):\n",
    "    #     raise ValueError(\"Could not find all required tags in the text.\")\n",
    "    # dialogue = dialogue_match.group(1).strip()\n",
    "    # reasoning = reasoning_match.group(1).strip()\n",
    "    # answer = answer_match.group(1).strip()\n",
    "\n",
    "    data = {\n",
    "        \"id_json\":i,\n",
    "\n",
    "        \"answer\": text_block.strip()\n",
    "    }\n",
    "\n",
    "    res=json.dumps(data)\n",
    "    with open(\"/DATA/rohan_kirti/niladri/dataset3/allexp/allexp_response.jsonl\", \"a\") as f:\n",
    "        f.write(res + \"\\n\")\n",
    "    return res\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "3d86ec3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load CSV\n",
    "def csv_load(i:int):\n",
    "    file_path = '/DATA/rohan_kirti/niladri/dataset3/conversation.csv'\n",
    "    file_path = '/DATA/rohan_kirti/niladri/dataset3/conversation.csv'\n",
    "    df = pd.read_csv(file_path)\n",
    "\n",
    "    conv_id = i\n",
    "    df = df[df['conversation_id'] == conv_id]\n",
    "\n",
    "    # Sort by turn number to ensure correct sequence\n",
    "    df.sort_values(by=\"turn_no\", inplace=True)\n",
    "\n",
    "    # Prepare conversation history\n",
    "    history = []\n",
    "    result = []\n",
    "\n",
    "    # Iterate through each row except the last one\n",
    "    for i in range(len(df)):\n",
    "        row = df.iloc[i]\n",
    "        speaker = row['speaker']\n",
    "        utterance = row['utterance']\n",
    "        result.append(f\"{speaker}: {utterance}\")\n",
    "\n",
    "    return result\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "35426e0d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "46"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result=list()\n",
    "for i in range(1,5):\n",
    "    res = csv_load(i)\n",
    "    result.extend(res)  # Use extend to flatten the list\n",
    "    \n",
    "len(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc9e8353",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User: Hi, I'm looking to get motor insurance for my new electric vehicle. It's a 2024 Tesla Model 3.\n",
      "Agent: Great choice! The Tesla Model 3 is an excellent vehicle. Since you've opted for an EV, are you particularly interested in coverage specific to electric vehicles, like battery protection?\n",
      "User: Yes, battery protection is definitely a concern. It's a big investment, and I want to make sure it's covered.\n",
      "Agent: Absolutely. The battery is the heart of your Tesla. With Tata AIG, you get rapid claims resolution combining thorough coverage with rapid claims resolution. It integrates technology with traditional risk management practices, ensuring that claims are processed quickly and effectively.\n",
      "User: What kind of coverage options do you have specifically for EVs?\n",
      "Agent: We offer a comprehensive plan that includes coverage for accidental damage, theft, and third-party liability. More importantly, we offer add-ons like Zero Depreciation Cover and Engine & Gearbox Protection. And our online policy management system makes everything simple for someone like you.\n",
      "User: Okay, that sounds pretty good. Can you give me a quote?\n",
      "Agent: Sure. For a 2024 Tesla Model 3 with comprehensive coverage including battery protection and roadside assistance, the annual premium would be approximately $2800. This includes a discount for purchasing the policy online.\n",
      "User: That's within my budget. What's the claim process like if I need to use it?\n",
      "Agent: The claim process is designed to be as smooth and hassle-free as possible. Accidents are unpredictable, but the financial stress doesn’t have to be. We offer prompt claims settlement, so you can focus on recovery, not paperwork. Our team is available 24/7 to guide you through the process.\n",
      "User: That sounds good. I appreciate the reassurance. I'm ready to proceed with the policy.\n",
      "User: Hi, I'm looking to get insurance for my bike. It's a 2022 Royal Enfield Interceptor 650.\n",
      "Agent: The Interceptor 650 is an excellent bike! As a professional, do you use it mainly for leisure or commuting?\n",
      "User: Mostly for weekend rides, so leisure I guess.\n",
      "Agent: Since you are not a daily commuter, you might not need the most expensive plan out there. However, accidents can still happen, especially on leisure rides. Our comprehensive plan ensures you're financially protected from unexpected damages.\n",
      "User: What's covered in a comprehensive plan?\n",
      "Agent: It covers damages to your bike from accidents, theft, natural disasters, and third-party liabilities. Tata AIG is designed to address modern vehicle risks, this insurance product combines thorough coverage with rapid claims resolution.\n",
      "User: Okay, that sounds good. What about roadside assistance?\n",
      "Agent: Roadside assistance is available as part of our comprehensive coverage and offers help if you experience vehicle breakdowns, requiring towing services, tire changes, fuel delivery, or emergency repairs while on the road.\n",
      "User: How much would this cost me?\n",
      "Agent: For a 2022 Royal Enfield Interceptor 650 with comprehensive coverage, it would be approximately $950 annually. And since you only use your bike for leisure, we can offer a discount making it $850 annually.\n",
      "User: That sounds reasonable. I'm happy to pay $850 annually. Please tell me more about the claim process.\n",
      "Agent: It is one of our most sought after policies because it offers excellent value for weekend riders\n",
      "Agent: Our claim process is designed to be user-friendly. You can initiate a claim online or through our mobile app. We also offer paperless claim settlement, reducing paperwork and expediting claim settlements in an environmentally friendly manner.\n",
      "User: That sounds convenient. Okay, I'm interested. What's the next step?\n",
      "User: Hi, I'm looking for a motor insurance policy for my bike. It's a 2022 Royal Enfield Classic 350.\n",
      "Agent: Okay, a Royal Enfield Classic 350 is a great bike! Since you're a professional and likely value your time, would you prefer a policy with quick claim settlements or are you more focused on the lowest possible premium?\n",
      "User: I value my time. Quick claim settlement is important to me.\n",
      "Agent: I understand. As a professional, you're busy and don't want to be stuck dealing with lengthy claim processes. Bajaj Allianz emphasizes reliability and efficiency, offering extensive vehicle protection while ensuring that claims are settled quickly.\n",
      "User: What does this policy typically cover?\n",
      "Agent: It provides Comprehensive Coverage, Liability Coverage, access to our Network Garages, and excellent Customer Support. This is one of our most popular plans—most of our clients go for it because it offers complete peace of mind for daily drivers\n",
      "User: Okay. What would be the premium for this policy?\n",
      "Agent: For a 2022 Royal Enfield Classic 350, the annual premium would be around $850. This includes quick claim settlements.\n",
      "User: That sounds good, can I get paperless claim settlement as part of the deal.\n",
      "Agent: Yes, our policy includes paperless claim settlement, streamlining the claims process through digital submissions and verifications, reducing paperwork and expediting claim settlements in an environmentally friendly manner.\n",
      "User: Okay, I'm interested. Can you send me the policy details?\n",
      "User: Hi, I am interested in getting motor insurance for my bike. I just bought a new 2024 Royal Enfield Classic 350.\n",
      "Agent: Congratulations on your new Royal Enfield Classic 350! That's a fantastic bike. As a professional, you value efficiency. ICICI Lombard's plan with Cashless Repairs helps you cut unexpected costs and avoid downtime. It’s a smart choice if you value minimal out-of-pocket expense.\n",
      "User: Cashless repairs sound good. Does the insurance cover theft as well?\n",
      "Agent: Yes, it does. Our comprehensive policy covers theft, accidents, and any damages from natural disasters. We also offer roadside assistance in case you ever get stranded.\n",
      "User: That’s reassuring. What’s the claim process like if something happens?\n",
      "Agent: With IFFCO Tokio, you’re choosing a provider known for its customer-first approach and streamlined claims resolution. Their policy ensures clarity and speed during stressful times like accidents or thefts.\n",
      "User: Okay, that sounds pretty good. What would the premium be for the comprehensive policy?\n",
      "Agent: For a 2024 Royal Enfield Classic 350 with comprehensive coverage, the premium would be around $950 per year. This includes coverage for theft, accidents, and natural disasters, as well as roadside assistance.\n",
      "User: That sounds reasonable. Let me think about it.\n",
      "Agent: Consider the peace of mind knowing you're fully protected. Accidents are unpredictable, and the financial strain can be significant. Bajaj Allianz offers prompt claims settlement, allowing you to focus on recovery, not paperwork.\n"
     ]
    }
   ],
   "source": [
    "i=1\n",
    "for sentence in result:\n",
    "    persu=persuassion_expert(sentence)\n",
    "    sentiment_output = sentiment_expert(sentence)\n",
    "    keyterms_output = keyterms_expert(sentence)\n",
    "    intent_output = intent_expert(sentence)\n",
    "    \n",
    "    #5 extra experts\n",
    "    pos_tag = pos(sentence)\n",
    "    ner_tag = ner(sentence)\n",
    "    corefer= co_reference(sentence)\n",
    "    detect= detect_language(sentence)\n",
    "    segment= topic_segment(sentence)\n",
    "    \n",
    "    final_output = generate_combined_analysis(sentence, intent_output, keyterms_output, persu, sentiment_output,\n",
    "                                              pos_tag, ner_tag, corefer, detect, segment)\n",
    "    res = convert_structured_to_jsonl(final_output,i)\n",
    "    i+=1\n",
    "    print(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "563055fb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "57a7216d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaned data written to /DATA/rohan_kirti/niladri/dataset3/allexp/cleaned_output.jsonl\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import re\n",
    "\n",
    "# Function to clean markdown and formatting from text\n",
    "def clean_text(text):\n",
    "    # Remove markdown symbols and line breaks\n",
    "    cleaned = re.sub(r'[*`_>#\\\\\\-\\r\\n]+', ' ', text)\n",
    "    cleaned = re.sub(r'\\s+', ' ', cleaned)  # Collapse multiple spaces into one\n",
    "    return cleaned.strip()\n",
    "\n",
    "# Input and output file paths\n",
    "input_file = \"/DATA/rohan_kirti/niladri/dataset3/allexp/allexp_response.jsonl\"   # Replace with your actual input filename\n",
    "output_file = \"/DATA/rohan_kirti/niladri/dataset3/allexp/cleaned_output.jsonl\"\n",
    "\n",
    "# Process each line\n",
    "with open(input_file, \"r\", encoding=\"utf-8\") as infile, open(output_file, \"w\", encoding=\"utf-8\") as outfile:\n",
    "    for line in infile:\n",
    "        data = json.loads(line)\n",
    "        data[\"answer\"] = clean_text(data[\"answer\"])\n",
    "        outfile.write(json.dumps(data) + \"\\n\")\n",
    "\n",
    "print(f\"Cleaned data written to {output_file}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8551738",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
