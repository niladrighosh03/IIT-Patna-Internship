{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7342f750",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-17 17:42:20.047463: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-06-17 17:42:20.060655: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2025-06-17 17:42:20.077839: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2025-06-17 17:42:20.082698: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-06-17 17:42:20.095260: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-06-17 17:42:21.072862: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a560aca7a6e94037bf47817f47ed871f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
    "\n",
    "# Load Gemma model\n",
    "model_id = \"google/gemma-2b-it\"  # or \"google/gemma-7b-it\" if you have GPU resources\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_id, device_map=\"auto\", torch_dtype=torch.float16)\n",
    "\n",
    "# Set up text generation pipeline\n",
    "generator = pipeline(\"text-generation\", model=model, tokenizer=tokenizer, max_new_tokens=50)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "902886b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gemma_extract_keywords(sentence):\n",
    "    prompt = f\"\"\"\n",
    "    Extract the 4 to 5 most important single-word keywords from the following sentence, focusing specifically on the Logical persuasion strategy.\n",
    "\n",
    "    Logical persuasion includes reasoning, evidence, facts, statistics, cause-effect relationships, and structured arguments.\n",
    "\n",
    "    Return only the keywords as a comma-separated list, with no explanation or extra text.\n",
    "    Important: Do not include product names, brand names, place names, or proper nouns.\n",
    "\n",
    "\n",
    "    Sentence: \"{sentence}\"\n",
    "    \"\"\"\n",
    "    try:\n",
    "        output = generator(prompt, do_sample=False)[0][\"generated_text\"]\n",
    "        keyword_line = output.split(\"Keywords:\")[-1].strip().split(\"\\n\")[0]\n",
    "        keywords = [k.strip() for k in keyword_line.split(\",\") if k.strip()]\n",
    "        return keywords[:5] + [\"\"] * (5 - len(keywords))\n",
    "    except Exception as e:\n",
    "        print(f\"Error on: {sentence[:40]}... -> {e}\")\n",
    "        return [\"\"] * 5\n",
    "    \n",
    "def extract_keywords(sentence):\n",
    "    prompt = f\"\"\"\n",
    "You are a helpful assistant skilled in persuasion strategies.\n",
    "\n",
    "Extract 4 to 5 **single-word** keywords from the following sentence using the *Logical* persuasion strategy.\n",
    "\n",
    "Logical persuasion includes: reasoning, evidence, facts, statistics, cause-effect relationships, structured arguments.\n",
    "\n",
    "Do NOT include product names, brand names, or places.\n",
    "\n",
    "Only return keywords as a comma-separated list.\n",
    "\n",
    "Sentence: \"{sentence}\"\n",
    "\n",
    "Keywords:\n",
    "\"\"\"\n",
    "    try:\n",
    "        output = generator(prompt, do_sample=False)[0][\"generated_text\"]\n",
    "        keyword_line = output.split(\"Keywords:\")[-1].strip().split(\"\\n\")[0]\n",
    "        keywords = [k.strip() for k in keyword_line.split(\",\") if k.strip()]\n",
    "        return keywords[:5] + [\"\"] * (5 - len(keywords))\n",
    "    except Exception as e:\n",
    "        print(f\"Error on: {sentence[:40]}... -> {e}\")\n",
    "        return [\"\"] * 5\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57461b58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# hugging face token= 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"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "68747149",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-17 18:04:08.534661: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-06-17 18:04:08.547509: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2025-06-17 18:04:08.563758: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2025-06-17 18:04:08.568668: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-06-17 18:04:08.580487: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-06-17 18:04:09.486559: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f9b0f7ced8d2425baa4cf16b46c83121",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.48, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoProcessor, AutoModelForImageTextToText\n",
    "import torch\n",
    "\n",
    "# Load medgemma-4b-it model and processor (chat format)\n",
    "model_id = \"google/medgemma-4b-it\"\n",
    "\n",
    "model = AutoModelForImageTextToText.from_pretrained(\n",
    "    model_id,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    device_map=\"auto\",\n",
    ")\n",
    "processor = AutoProcessor.from_pretrained(model_id)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6af90bf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def extract_keywords(sentence):\n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": [{\"type\": \"text\", \"text\": \"You are an assistant skilled in logical *persuasion strategy*.\"}]\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [{\n",
    "                \"type\": \"text\",\n",
    "                \"text\": f\"\"\"Extract the *4 to 5 most important single-word keywords* from the following sentence, focusing specifically on the *Logical persuasion strategy*.\n",
    "\n",
    "\n",
    "    Return only the keywords as a comma-separated list, with no explanation or extra text.\n",
    "    Important: Do not include product names, brand names, place names, or proper nouns.\n",
    "\n",
    "    Sentence: \"{sentence}\"\n",
    "    \"\"\"\n",
    "            }]\n",
    "        }\n",
    "    ]\n",
    "\n",
    "\n",
    "    try:\n",
    "        # Prepare input for generation\n",
    "        inputs = processor.apply_chat_template(\n",
    "            messages, add_generation_prompt=True, tokenize=True,\n",
    "            return_dict=True, return_tensors=\"pt\"\n",
    "        ).to(model.device, dtype=torch.bfloat16)\n",
    "\n",
    "        input_len = inputs[\"input_ids\"].shape[-1]\n",
    "\n",
    "        # Generate output\n",
    "        with torch.inference_mode():\n",
    "            output = model.generate(**inputs, max_new_tokens=50, do_sample=False)\n",
    "            output = output[0][input_len:]\n",
    "\n",
    "        # Decode and clean response\n",
    "        decoded = processor.decode(output, skip_special_tokens=True).strip()\n",
    "        keywords = [kw.strip() for kw in decoded.split(\",\") if kw.strip()]\n",
    "        return keywords[:5] + [\"\"] * (5 - len(keywords))\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error on: {sentence[:40]}... -> {e}\")\n",
    "        return [\"\"] * 5\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f89d9ce4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/DATA/rohan_kirti/niladri\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " conversation1.csv   jupyter.log  'key_gemma copy.ipynb'   key_gemma.ipynb\n"
     ]
    }
   ],
   "source": [
    "!pwd\n",
    "!ls\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "05fb6c91",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:1 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['Assistance', 'Services', 'Benefits', 'Features', '']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "extract_keywords(\"Yes, HDFC ERGO includes Roadside Assistance with services like towing, jump-start, flat tire help, and fuel delivery.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c73d5fc6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/DATA/rohan_kirti/miniconda3/envs/salesbot/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:634: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.95` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n",
      "/DATA/rohan_kirti/miniconda3/envs/salesbot/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:651: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `64` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.\n",
      "  warnings.warn(\n",
      "Setting `pad_token_id` to `eos_token_id`:1 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:1 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Recommend', 'Insurance', 'Logical', '', '']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:1 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Benefits', 'Policy', 'Assistance', 'Depreciation', '']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:1 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Policy', 'Premium', 'Include', 'What', '']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:1 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Logic', 'Reason', 'Evidence', 'Value', 'Cost']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:1 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Policy', 'Assistance', 'Roadside', 'Logic', '']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:1 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Assistance', 'Services', 'Benefits', 'Features', '']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:1 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Zero Depreciation', '', '', '', '']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:1 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:1 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Value', 'Claim', 'Depreciation', 'Cost', '']\n",
      "['Easy', '', '', '', '']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:1 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Claim', 'Process', 'Tracking', 'Network', '']\n",
      "['Benefit', '', '', '', '']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:1 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:1 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Bonus', 'Premium', 'Reduce', 'Qualify', '']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:1 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Cover\\nDamage\\nPolicy\\nEngine', '', '', '', '']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:1 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Covered', 'Recommended', 'Opt', 'Protection', '']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:1 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Persuasion', 'Logic', 'Reason', 'Argument', 'Evidence']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:1 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Guide', 'Quote', 'Steps', 'Prepare', '']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:1 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Recommend', 'Insurance', 'Persuasion', 'Logical', '']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:1 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Benefits', 'Policy', 'Assistance', 'Depreciation', '']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:1 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Policy', 'Premium', 'Include', 'What', '']\n",
      "['Logic', 'Reason', 'Evidence', 'Value', 'Risk']\n",
      " Output saved to 'logical_keywords_output.csv'\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"conversation1.csv\")  \n",
    "keywords_list = []\n",
    "for _, row in df.iterrows():\n",
    "    if str(row[\"P-Strategy\"]).strip().lower() == \"logical\":\n",
    "        keywords = extract_keywords(row[\"utterance\"])\n",
    "        print(keywords)\n",
    "    else:\n",
    "        keywords = [\"\"] * 5\n",
    "        # print(\"hi\")\n",
    "    keywords_list.append(keywords)\n",
    "\n",
    "keyword_df = pd.DataFrame(keywords_list, columns=[\"keyword1\", \"keyword2\", \"keyword3\", \"keyword4\", \"keyword5\"])\n",
    "df_with_keywords = pd.concat([df, keyword_df], axis=1)\n",
    "\n",
    "df_with_keywords.to_csv(\"gemma_logical_keywords1.csv\", index=False)\n",
    "print(\" Output saved to 'logical_keywords_output.csv'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9f8af54",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dda327d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1ebcb1d9",
   "metadata": {},
   "source": [
    "## Gemma 4b-it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c8d36c0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-17 20:20:03.130110: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-06-17 20:20:03.143838: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2025-06-17 20:20:03.160887: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2025-06-17 20:20:03.165982: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-06-17 20:20:03.178254: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-06-17 20:20:04.085949: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3f94c73007bf4e5cbaf36f7468339caf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.48, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoProcessor, Gemma3ForConditionalGeneration\n",
    "import torch\n",
    "\n",
    "# Use gemma-3-4b-it (text-only version)\n",
    "model_id = \"google/gemma-3-4b-it\"\n",
    "\n",
    "# Load model and processor\n",
    "model = Gemma3ForConditionalGeneration.from_pretrained(\n",
    "    model_id,\n",
    "    device_map=\"auto\"\n",
    ").eval()\n",
    "\n",
    "processor = AutoProcessor.from_pretrained(model_id)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c8d7832",
   "metadata": {},
   "outputs": [],
   "source": [
    "def logical_extract_keywords(sentence):\n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": [{\"type\": \"text\", \"text\": \"You are an assistant skilled in logical *persuasion strategy*.\"}]\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [{\n",
    "                \"type\": \"text\",\n",
    "                \"text\": f\"\"\"Extract the *4 to 5 most important single-word keywords* from the following sentence, focusing specifically on the *Logical persuasion strategy*.\n",
    "\n",
    "Return only the keywords as a comma-separated list, with no explanation or extra text.\n",
    "Important: Do not include product names, brand names, place names, or proper nouns.\n",
    "\n",
    " Examples:\n",
    "    Sentence: \"I have a 2021 Honda Amaze. What insurance would you recommend?\"\n",
    "    Output: insurance, recommend\n",
    "\n",
    "    Sentence: \"It includes own damage, third-party liability, theft, natural disasters, and more. The premium is approx $1176 per year, based on IDV.\"\n",
    "    Output: damage, theft, disaster, premium\n",
    "\n",
    "    Sentence: \"This plan provides better coverage and lower premium compared to the previous one.\"\n",
    "    Output: coverage, premium, comparison\n",
    "\n",
    "    Sentence: \"The repair costs are significantly reduced with this policy.\"\n",
    "    Output: repair, cost, policy\n",
    "\n",
    "    Now, extract from the following:\n",
    "\n",
    "Sentence: \"{sentence}\"\n",
    "\"\"\"\n",
    "            }]\n",
    "        }\n",
    "    ]\n",
    "\n",
    "\n",
    "\n",
    "    # messages = [\n",
    "    #     {\n",
    "    #         \"role\": \"system\",\n",
    "    #         \"content\": [\n",
    "    #             {\n",
    "    #                 \"type\": \"text\",\n",
    "    #                 \"text\": \"You are an assistant skilled in identifying keywords used in logical persuasion. Logical persuasion is based on facts, statistics, reasoning, cost-benefit analysis, and objective evaluations. It avoids emotional appeals or vague claims, and instead focuses on evidence, consequences, comparisons, functionality, efficiency, and measurable impact.\"\n",
    "    #             }\n",
    "    #         ]\n",
    "    #     },\n",
    "    #     {\n",
    "    #         \"role\": \"user\",\n",
    "    #         \"content\": [\n",
    "    #             {\n",
    "    #                 \"type\": \"text\",\n",
    "    #                 \"text\": \"\"\"Your task is to extract the *4 to 5 most important single-word keywords* from the sentence below. Focus strictly on the *Logical persuasion strategy*.\n",
    "\n",
    "    # Logical persuasion involves factual or analytical reasoning—such as cost, risk, benefit, performance, features, warranty, coverage, or other evidence-based elements. Avoid including emotional, subjective, or brand-related words.\n",
    "\n",
    "    #  Important Instructions:\n",
    "    # - Return only the keywords as a comma-separated list.\n",
    "    # - Do not include product names, brand names, places, or proper nouns.\n",
    "    # - Only pick nouns, verbs, or adjectives related to logical decision-making or analysis.\n",
    "\n",
    "    #  Examples:\n",
    "    # Sentence: \"I have a 2021 Honda Amaze. What insurance would you recommend?\"\n",
    "    # Output: insurance, recommend\n",
    "\n",
    "    # Sentence: \"It includes own damage, third-party liability, theft, natural disasters, and more. The premium is approx $1176 per year, based on IDV.\"\n",
    "    # Output: damage, theft, disaster, premium\n",
    "\n",
    "    # Sentence: \"This plan provides better coverage and lower premium compared to the previous one.\"\n",
    "    # Output: coverage, premium, comparison\n",
    "\n",
    "    # Sentence: \"The repair costs are significantly reduced with this policy.\"\n",
    "    # Output: repair, cost, policy\n",
    "\n",
    "    # Now, extract from the following:\n",
    "\n",
    "    # Sentence: \"{sentence}\"\n",
    "    # \"\"\"\n",
    "    #             }\n",
    "    #         ]\n",
    "    #     }\n",
    "    # ]\n",
    "\n",
    "\n",
    "    try:\n",
    "        # Prepare input\n",
    "        inputs = processor.apply_chat_template(\n",
    "            messages,\n",
    "            add_generation_prompt=True,\n",
    "            tokenize=True,\n",
    "            return_dict=True,\n",
    "            return_tensors=\"pt\"\n",
    "        ).to(model.device, dtype=torch.bfloat16)\n",
    "\n",
    "        input_len = inputs[\"input_ids\"].shape[-1]\n",
    "\n",
    "        # Generate output\n",
    "        with torch.inference_mode():\n",
    "            output = model.generate(**inputs, max_new_tokens=50, do_sample=False)\n",
    "            output = output[0][input_len:]\n",
    "\n",
    "        # Decode and clean output\n",
    "        decoded = processor.decode(output, skip_special_tokens=True).strip()\n",
    "        keywords = [kw.strip() for kw in decoded.split(\",\") if kw.strip()]\n",
    "        return keywords[:5] + [\"\"] * (5 - len(keywords))\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error on: {sentence[:40]}... -> {e}\")\n",
    "        return [\"\"] * 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ab9bc17",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/DATA/rohan_kirti/miniconda3/envs/salesbot/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:634: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.95` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n",
      "/DATA/rohan_kirti/miniconda3/envs/salesbot/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:651: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `64` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['insurance', 'recommend', '', '', '']\n",
      "['policies', 'benefits', 'assistance', 'depreciation', '']\n",
      "['policy', 'premium', 'include', '', '']\n",
      "['damage', 'theft', 'disaster', 'premium', '']\n",
      "['assistance', 'policy', 'roadside', '', '']\n",
      "['assistance', 'services', 'delivery', 'help', '']\n",
      "['add-ons', 'depreciation', '', '', '']\n",
      "['Depreciation', 'claim', 'cost', 'valuable', '']\n",
      "['process', 'easy', 'claim', '', '']\n",
      "['process', 'tracking', 'network', 'repairs', '']\n",
      "['claimed', 'benefit', '', '', '']\n",
      "['qualify', 'premium', 'bonus', 'reduce', '']\n",
      "['damage', 'policy', 'cover', '', '']\n",
      "['covered', 'protection', 'recommended', '', '']\n",
      "['interest', 'plan', '', '', '']\n",
      "['quote', 'purchase', 'guide', '', '']\n",
      "['insurance', 'recommend', '', '', '']\n",
      "['policies', 'benefits', 'assistance', 'depreciation', '']\n",
      "['policy', 'premium', 'include', '', '']\n",
      " Output saved to 'gemma_logical_keywords2.csv'\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"conversation1.csv\")  \n",
    "keywords_list = []\n",
    "for _, row in df.iterrows():\n",
    "    if str(row[\"P-Strategy\"]).strip().lower() == \"logical\":\n",
    "        keywords = logical_extract_keywords(row[\"utterance\"])\n",
    "        print(keywords)\n",
    "    elif str(row[\"P-Strategy\"]).strip().lower() == \"emotional\":\n",
    "        keywords = [\"\"] * 5\n",
    "        # print(\"hi\")\n",
    "    keywords_list.append(keywords)\n",
    "\n",
    "keyword_df = pd.DataFrame(keywords_list, columns=[\"keyword1\", \"keyword2\", \"keyword3\", \"keyword4\", \"keyword5\"])\n",
    "df_with_keywords = pd.concat([df, keyword_df], axis=1)\n",
    "\n",
    "df_with_keywords.to_csv(\"gemma_logical_keywords2.csv\", index=False)\n",
    "print(\" Output saved to 'gemma_logical_keywords2.csv'\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
